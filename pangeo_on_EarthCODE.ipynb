{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e5f5581-d3a0-4664-97d1-bfc55b46d146",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22680c10-8e98-4cc9-a091-f54c02a3e6e3",
   "metadata": {},
   "source": [
    "Structure:\n",
    "\n",
    "1. Introducing the EDC Pangeo Cloud Platform and Dask\n",
    "2. Accessing the EarthCODE Open Science Catalog Programatically via STAC API\n",
    "3. Accessing Data on the OSC - Introducing Zarr/Chunking\n",
    "4. Introducing Xarray and Distributed Computing with Dask\n",
    "5. Example\n",
    "6. Saving your work and Publishing to the EarthCODE Catalog\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bfdc85-21d7-4791-b98a-1b2c73cfe4ef",
   "metadata": {},
   "source": [
    "TODO's: \n",
    "- [ ] Make repo\n",
    "- [ ] Try to load data in a more stac friendly way - odc.load / stackstac\n",
    "- [ ] Tidy up and correct markdown/more comments\n",
    "- [ ] References need to be good and to make it a point during the talk\n",
    "- [ ] Publishing - go end-to-end\n",
    "- [ ] Fix imports\n",
    "- [ ] Read in saved polygon from file rather than pasted\n",
    "- [ ] Dask needs to be properly instantiated\n",
    "- [ ] All needs to run on EDC\n",
    "- [ ] Projections need to be fixed (see hv plot with two plots at the bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe9f83-5f08-473f-a2d0-f35d76d199b9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e020884b-2276-4bde-92e5-1c30fb689385",
   "metadata": {},
   "source": [
    "> Cite As\n",
    "Alonso, Lazaro, Gans, Fabian, Karasante, Ilektra, Ahuja, Akanksha, Prapas, Ioannis, Kondylatos, Spyros, Papoutsis, Ioannis, Panagiotou, Eleannna, Michail, Dimitrios, Cremer, Felix, Weber, Ulrich, & Carvalhais, Nuno. (2022). SeasFire Cube: A Global Dataset for Seasonal Fire Modeling in the Earth System (0.4) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.13834057\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22168f3-4e2e-4835-85b0-98fdb2d96d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stackstac\n",
    "import xarray\n",
    "import pystac\n",
    "import dask.distributed\n",
    "import rasterio\n",
    "from odc.stac import stac_load\n",
    "import odc\n",
    "import os\n",
    "from odc.stac import configure_rio\n",
    "from pystac.extensions.datacube import DatacubeExtension\n",
    "from odc.stac import configure_s3_access, stac_load\n",
    "from pystac.extensions.storage import StorageExtension\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import xarray as xr\n",
    "from ipyleaflet import Map, Polygon\n",
    "from shapely import geometry\n",
    "from shapely.geometry import shape, box\n",
    "from ipyleaflet import Map, GeoJSON\n",
    "\n",
    "from pystac_client import Client as pystac_client\n",
    "from odc.stac import configure_rio, stac_load\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5273b-3793-46be-a105-d62e3876bf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsg = 4326\n",
    "# make the other one use this way\n",
    "# 1. Your GeoJSON feature as a Python dict\n",
    "feature = {\n",
    "  \"type\": \"Feature\",\n",
    "  \"properties\": {\n",
    "    \"label\": \"Polygon 1\",\n",
    "    \"color\": \"red\"\n",
    "  },\n",
    "  \"geometry\": {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "      [\n",
    "        [\n",
    "          -7.661233095720918,\n",
    "          41.371255582375056\n",
    "        ],\n",
    "        [\n",
    "          -7.655930125371327,\n",
    "          40.98784631510793\n",
    "        ],\n",
    "        [\n",
    "          -6.819489589712093,\n",
    "          41.0006005650834\n",
    "        ],\n",
    "        [\n",
    "          -6.8371844752309325,\n",
    "          41.38919212862254\n",
    "        ],\n",
    "        [\n",
    "          -7.661233095720918,\n",
    "          41.371255582375056\n",
    "        ]\n",
    "      ]\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "poly = shape(feature[\"geometry\"])\n",
    "bbox = list(poly.bounds)\n",
    "center = ((bbox[1] + bbox[3]) / 2.0, (bbox[0] + bbox[2]) / 2.0)\n",
    "m = Map(center=center, zoom=10)\n",
    "# Create polygon from lists of points\n",
    "polygon = Polygon(\n",
    "    locations=[\n",
    "        (bbox[1], bbox[0]),\n",
    "        (bbox[3], bbox[0]),\n",
    "        (bbox[3], bbox[2]),\n",
    "        (bbox[1], bbox[2]),\n",
    "    ],\n",
    "    color=\"green\",\n",
    "    fill_color=\"green\",\n",
    ")\n",
    "\n",
    "# Add the polygon to the map\n",
    "m.add(polygon)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb53fdf2-baa6-4634-90bd-e4d58047a0df",
   "metadata": {},
   "source": [
    "# Introducing the EDC Pangeo Cloud Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3644266a-6137-4806-9070-5ace09114025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed.client import _global_clients\n",
    "for client in list(_global_clients.values()):\n",
    "    client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483087e-b4cb-45d0-9d7a-c6c2d4b3b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xcube --> open with xcube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0202b9a-f612-4507-adff-3b28dd2d9775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9065729-1c81-4870-8144-c586347b2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2cf45-f9d6-406f-8fbd-32900265af08",
   "metadata": {},
   "source": [
    "### What is [Dask](https://docs.dask.org/) ?\n",
    "\n",
    "**Dask** scales the existing Python ecosystem: with very or no changes in your code, you can speed-up computation using Dask or process bigger than memory datasets.\n",
    "\n",
    "- Dask is a flexible library for parallel computing in Python.\n",
    "- It is widely used for handling large and complex Earth Science datasets and speed up science.\n",
    "- Dask is powerful, scalable and flexible. It is the leading platform today for data analytics at scale.\n",
    "- It scales natively to clusters, cloud, HPC and bridges prototyping up to production.\n",
    "- The strength of Dask is that is scales and accelerates the existing Python ecosystem e.g. Numpy, Pandas and Scikit-learn with few effort from end-users.\n",
    "\n",
    "It is interesting to note that at first, [Dask has been created to handle data that is larger than memory, on a single computer](https://coiled.io/blog/history-dask/). It then was extended with Distributed to compute data in parallel over clusters of computers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9896b-b3a5-4830-986a-f57e39a83f25",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b165b8f-b690-41dd-ae39-89d3e72617ee",
   "metadata": {},
   "source": [
    "### What is a Dask Distributed cluster ?\n",
    "\n",
    "A Dask Distributed cluster is made of two main components:\n",
    "\n",
    "- a Scheduler, responsible for handling computations graph and distributing tasks to Workers.\n",
    "- One or several (up to 1000s) Workers, computing individual tasks and storing results and data into distributed memory (RAM and/or worker's local disk).\n",
    "\n",
    "A user usually needs __Client__ and __Cluster__ objects as shown below to use Dask Distributed.    \n",
    "\n",
    "![Dask Distributed Cluster](https://user-images.githubusercontent.com/306380/66413985-27111600-e9be-11e9-9995-8f418ff48f8a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d4cdd-2f6f-4033-a612-20262685def5",
   "metadata": {},
   "source": [
    "#### Where can we deploy a Dask distributed cluster?\n",
    "\n",
    "\n",
    "[Dask distributed clusters can be deployed on your laptop or on distributed infrastructures (Cloud, HPC centers, Hadoop, etc.).] - (https://docs.dask.org/en/stable/deploying.html)  Dask distributed `Cluster` object is responsible of deploying and scaling a Dask Cluster on the underlying resources.\n",
    "\n",
    "EDC has one such deployment\n",
    "\n",
    "\n",
    "![Dask Cluster deployment](https://docs.dask.org/en/stable/_images/dask-cluster-manager.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d39aa-7ff1-4b97-9f51-9ff72efc8a85",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "A Dask `Cluster` can be created on a single machine (for instance your laptop) e.g. there is no need to have dedicated computational resources. However, speedup will only be limited to your single machine resources if you do not have dedicated computational resources!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8b5dcd-e6dc-4c02-bd43-0023f1e4ee8b",
   "metadata": {},
   "source": [
    "## Scaling your Computation using Dask Gateway.\n",
    "\n",
    "For this workshop, according to the Pangeo EDC deployment, you will learn how to use Dask Gateway to manage Dask clusters over Kubernetes, allowing to run our data analysis in parallel e.g. distribute tasks across several workers.\n",
    "\n",
    "Lets set up your Dask cluster through Dask Gateway.  \n",
    "As Dask Gateway is configured by default on this ifnrastructure, you just need to execute the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b9f5a-e252-43b8-8740-73d162b13ece",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ff714-c514-40c7-b392-d243c9dc2d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "\n",
    "# client = Client()   # create a local dask cluster on the local machine.\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92534dd5-64ab-4413-aabb-6d880db640ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21ad6c9a-2432-42bc-aedd-6059d65231a4",
   "metadata": {},
   "source": [
    "Inspecting the `Cluster Info` section above gives us information about the created cluster: we have 2 or 4 workers and the same number of threads (e.g. 1 thread per worker). \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Go further</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li> You can also create a local cluster with the `LocalCluster` constructor and use `n_workers` \n",
    "        and `threads_per_worker` to manually specify the number of processes and threads you want to use. \n",
    "        For instance, we could use `n_workers=2` and `threads_per_worker=2`.  </li>\n",
    "        <li> This is sometimes preferable (in terms of performance), or when you run this tutorial on your PC, \n",
    "        you can avoid dask to use all your resources you have on your PC!  </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0581ff1-c059-49f9-8f49-f700974bc4fe",
   "metadata": {},
   "source": [
    "### Dask distributed Client\n",
    " \n",
    "The Dask distributed `Client` is what allows you to interact with Dask distributed Clusters. When using Dask distributed, you always need to create a `Client` object. Once a `Client` has been created, it will be used by default by each call to a Dask API, even if you do not explicitly use it.\n",
    "\n",
    "No matter the Dask API (e.g. Arrays, Dataframes, Delayed, Futures, etc.) that you use, under the hood, Dask will create a Directed Acyclic Graph (DAG) of tasks by analysing the code. Client will be responsible to submit this DAG to the Scheduler along with the final result you want to compute. The Client will also gather results from the Workers, and aggregate it back in its underlying Python process.\n",
    "\n",
    "Using `Client()` function with no argument, you will create a local Dask cluster with a number of workers and threads per worker corresponding to the number of cores in the 'local' machine. Here, during the workshop, we are running this notebook in Pangeo EOSC cloud deployment, so the 'local' machine is the jupyterlab you are using at the Cloud, and the number of cores is the number of cores on the cloud computing resources you've been given (not on your laptop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6054176-3bba-474e-9b9f-cf4d89197a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78f0a8f5-8b22-4f75-9c8d-957f8a58c5db",
   "metadata": {},
   "source": [
    "### Dask Dashboard\n",
    "\n",
    "Dask comes with a really handy interface: the Dask Dashboard. It is a web interface that you can open in a separated tab of your browser (but not with he link above, you've got to use Jupyterlabs proxy: https://pangeo-foss4g.vm.fedcloud.eu/jupyterhub/user/_yourusername_/proxy/8787/status).\n",
    "\n",
    "We will learn here how to use it through [dask jupyterlab extension](https://github.com/dask/dask-labextension).  \n",
    "\n",
    "To use Dask Dashboard through jupyterlab extension on Pangeo EDC infrastructure,\n",
    "you will just need too look at the html link you have for your jupyterlab, and Dask dashboard port number, as highlighted in the figure below.\n",
    "\n",
    "![Dash Board link](./static/dashboardlink.png)\n",
    "![Dash lab](./static/dasklab.png)\n",
    "\n",
    "Then click the orange icon indicated in the above figure, and type 'your' dashboard link (normally, you just need to replace 'todaka' to 'your username').  \n",
    "\n",
    "\n",
    "You can click several buttons indicated with blue arrows in above figures, then drag and drop to place them as your convenience.  \n",
    "\n",
    "![Example dask lab](./static/exampledasklab.png)\n",
    "\n",
    "\n",
    "It's really helpfull to understand your computation and how it is distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf242d8-b3dd-4bde-8980-7bc6d35ffa4f",
   "metadata": {},
   "source": [
    "# Accessing the EarthCODE Catalog\n",
    "\n",
    "This section introduces [STAC](https://stacspec.org/), the SpatioTemporal Asset Catalog. STAC provides a standardized way to structure metadata about spatialotemporal data. The STAC community are building APIs and tools on top of this structure to make working with spatiotemporal data easier.\n",
    "\n",
    "\n",
    "Users of STAC will interact most often with Collections and Items (there's also Catalogs, which group together collections). A Collection is just a collection of items, plus some additional metadata like the license and summaries of what's available on each item. You can view available collections on the EarthCODE catalog with\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e743c-5edb-4409-97b7-1cc189510874",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = pystac_client.open(\"https://catalog.osc.earthcode.eox.at/\")\n",
    "cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5956604-0fa6-4065-8e00-032dc0203b42",
   "metadata": {},
   "source": [
    "Now let's search and access the same data we were just looking at, SeasFire https://opensciencedata.esa.int/products/seasfire-cube/collection\n",
    "\n",
    "\n",
    "In the examples we've seen so far, we've just been given a STAC item. How do you find the items you want in the first place? That's where a STAC API comes in.\n",
    "\n",
    "A STAC API is some web service that accepts queries and returns STAC objects. The ability to handle queries is what differentiates a STAC API from a static STAC catalog, where items are just present on some file system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521fad68-4608-4b2c-a02a-25ef5dfd03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO SEARCH AND ACCESS SEASFIRE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56f1e3e2-a968-4660-a9c7-c86efba57dec",
   "metadata": {},
   "source": [
    "The collection points to another collection, which contains the actual data. The EarthCODE STAC extension describes some metadata that enrich the STAC collection https://github.com/stac-extensions/osc.\n",
    "\n",
    "![img.png](./EarthCODE-STAC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d362f6-d4cc-43eb-93a3-0180006bfd89",
   "metadata": {},
   "source": [
    "# Accessing Data from EarthCODE\n",
    "\n",
    "Loading the actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f183c8d-46e4-49f5-932e-af56234579c8",
   "metadata": {},
   "source": [
    "The SeasFire Cube is a scientific datacube for seasonal fire forecasting around the globe. It has been created for the SeasFire project, that adresses 'Earth System Deep Learning for Seasonal Fire Forecasting' and is funded by the European Space Agency (ESA)  in the context of ESA Future EO-1 Science for Society Call. It contains almost 20 years of data (2001-2021) in an 8-days time resolution and 0.25 degrees grid resolution. It has a diverse range of seasonal fire drivers. It expands from atmospheric and climatological ones to vegetation variables, socioeconomic and the target variables related to wildfires such as burned areas, fire radiative power, and wildfire-related CO2 emissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7c6bc-63d4-4419-a9d2-771d1b2d2323",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasfire = pystac.read_file(\n",
    "    \"https://s3.waw4-1.cloudferro.com/EarthCODE/Catalogs/seasfire/seasfire-cube_v0.4/catalog.json\"\n",
    ")\n",
    "\n",
    "for item in seasfire.get_items():\n",
    "    print(item)\n",
    "\n",
    "seasfire\n",
    "# https://s3.waw4-1.cloudferro.com/EarthCODE/Catalogs/seasfire/seasfire-cube_v0.4/seasfire-cube-v.0.4/seasfire-cube-v.0.4.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a0d49-7dc8-4b38-b90f-513d787a0fbe",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "When dealing with large data files or collections, it's often impossible to load all the data you want to analyze into a single computer's RAM at once. This is a situation where the Pangeo ecosystem can help you a lot. Xarray offers the possibility to work lazily on data __chunks__, which means pieces of an entire dataset. By reading a dataset in __chunks__ we can process our data piece by piece on a single computer and even on a distributed computing cluster using Dask (Cloud or HPC for instance).\n",
    "\n",
    "How we will process these 'chunks' in a parallel environment will be discussed in [dask_introduction](./dask_introduction.ipynb). The concept of __chunk__ will be explained here.\n",
    "\n",
    "When we process our data piece by piece, it's easier to have our input or ouput data also saved in __chunks__. [Zarr](https://zarr.readthedocs.io/en/stable/) is the reference library in the Pangeo ecosystem to save our Xarray multidimentional datasets in __chunks__.\n",
    "\n",
    "[Zarr](https://zarr.readthedocs.io/en/stable/) is not the only file format which uses __chunk__. We will also be using [kerchunk library](https://fsspec.github.io/kerchunk/) in this notebook to build a virtual __chunked__ dataset based on NetCDF files, and show how it optimizes the access and analysis of large datasets.\n",
    "\n",
    "The analysis is very similar to what we have done in previous episodes, however we will use data on a global coverage and not only on a small geographical area (e.g. Lombardia).\n",
    "\n",
    "### Data\n",
    "\n",
    "In this episode, we will be using Global Long Term Statistics (1999-2019) products provided by the [Copernicus Global Land Service](https://land.copernicus.eu/global/index.html) and access them through [S3-comptabile storage](https://en.wikipedia.org/wiki/Amazon_S3) ([OpenStack Object Storage \"Swift\"](https://wiki.openstack.org/wiki/Swift)) with a data catalog we have created and made publicly available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9069e4-6606-4457-8c5b-704b5b941d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasfire_cube = seasfire.get_item(\"seasfire-cube-v.0.4\")\n",
    "seasfire_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ce2c9-f02a-44a1-80a3-02e71dc2d9ee",
   "metadata": {},
   "source": [
    "## Assets\n",
    "STAC is a metadata standard. It doesn't really deal with data files directly. Instead, it links to the data files under the \"assets\" property.\n",
    "\n",
    "---\n",
    "\n",
    "The STAC catalog contains a collection for each Zarr store and there are collection-level assets that point to the location of the Zarr store. There are no items at all in this setup.\n",
    "\n",
    "In this scenario any STAC metadata exists purely for discovery and cannot be used for filtering or subsetting (see Future Work for more on that). To search the STAC catalog to find collections of interest you will use the Collection Search API Extension. Depending on the level of metadata that has been provided in the STAC catalog you can search by the name of the collection and possibly by the variables – exposed via the Data Cube Extension.\n",
    "\n",
    "Read straight to xarray\n",
    "Once you have found the collection of interest, the best approach for accessing the data is to construct the lazily-loaded data cube in xarray (or an xarray.DataTree if the Zarr store has more than one group) and filter from there.\n",
    "\n",
    "To do this you can use the zarr backend directly or you can use the stac backend to streamline even more. The stac backend is mostly useful if the STAC collection uses the xarray extension.\n",
    "\n",
    "Constructing the lazy data cube is likely to be very fast if there is a consolidated metadata file OR the data is in Zarr-3 and the Zarr metadata fetch is highly parallelized (read more).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c2630-2155-42dc-b21f-13b1440598e4",
   "metadata": {},
   "source": [
    "## What is a __chunk__\n",
    "\n",
    "If you look carefully to `LTS`, each Data Variable is a `dask.array` with a chunk size of `(15680, 40320)`. So basically accessing one data variable would load arrays of dimensions `(15680, 40320)` into the computer's RAM. You can see this information and more details by clicking the icon as indicated in the image below.\n",
    "\n",
    "![Dask.array](../figures/datasize.png)\n",
    "\n",
    "When you open one or several netCDF files with `open_mdfataset`, by default, the chunks correspond to the entire size of the variable data array read from each file. When you need to analyze large files, a computer's memory may not be sufficient anymore (see in this example, 2.36GiB for one chunk!).\n",
    "\n",
    "This is where understanding and using chunking correctly comes into play.\n",
    "\n",
    "\n",
    "__Chunking__ is splitting a dataset into small pieces. \n",
    "\n",
    "Original dataset is in one piece,  \n",
    "![Dask.array](../figures/notchunked.png)\n",
    "\n",
    "and we split it into several smaller pieces.  \n",
    "![Dask.array](../figures/chunked.png)\n",
    "\n",
    "We split it into pieces so that we can process our data block by block or __chunk__ by __chunk__.\n",
    "\n",
    "In our case, for the moment, the dataset is composed of several files, so already several pieces (or just one in the example above), and Xarray just creates one chunk for each variable of each file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317630c4-e066-494e-9cf5-a2dabaf197e1",
   "metadata": {},
   "source": [
    "Zarr format main characteristics are the following:\n",
    "\n",
    "- Every chunk of a Zarr dataset is stored as a single file (see x.y files in `ls -al test.zarr/nobs`)\n",
    "- Each Data array in a Zarr dataset has a two unique files containing metadata:\n",
    "  - .zattrs for dataset or dataarray general metadatas\n",
    "  - .zarray indicating how the dataarray is chunked, and where to find them on disk or other storage.\n",
    "  \n",
    "Zarr can be considered as an Analysis Ready, cloud optimized data (ARCO) file format, discussed in [data_discovery](./data_discovery.ipynb) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c27381-043a-4343-831e-95ae689cf5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "http_url = seasfire_cube.assets[\"data\"].href.replace(\n",
    "    \"s3://\",\n",
    "    f\"{seasfire_cube.properties['storage:schemes'][seasfire_cube.assets['data'].extra_fields['storage:refs'][0]]['platform'].rstrip('/')}/\",\n",
    ")\n",
    "\n",
    "ds = xarray.open_dataset(\n",
    "\thttp_url,\n",
    "\tengine='zarr',\n",
    "    chunks={},\n",
    "\tconsolidated=True\n",
    "\t# storage_options = {'token': 'anon'}\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343eb7b4-280e-478f-b301-2332102a9331",
   "metadata": {},
   "source": [
    "## What is xarray?\n",
    "\n",
    "Xarray introduces labels in the form of dimensions, coordinates and attributes on top of raw NumPy-like multi-dimensional arrays, which allows for a more intuitive, more concise, and less error-prone developer experience.\n",
    "\n",
    "### How is xarray structured?\n",
    "\n",
    "Xarray has two core data structures, which build upon and extend the core strengths of NumPy and Pandas libraries. Both data structures are fundamentally N-dimensional:\n",
    "\n",
    "- [DataArray](https://docs.xarray.dev/en/stable/generated/xarray.DataArray.html#xarray.DataArray) is the implementation of a labeled, N-dimensional array. It is an N-D generalization of a Pandas.Series. The name DataArray itself is borrowed from [Fernando Perez’s datarray project](http://fperez.org/py4science/datarray/), which prototyped a similar data structure.\n",
    "\n",
    "- [Dataset](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.html#xarray.Dataset) is a multi-dimensional, in-memory array database. It is a dict-like container of DataArray objects aligned along any number of shared dimensions, and serves a similar purpose in xarray as the pandas.DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085af36-d7a5-4c73-9d35-525eef0d1ae4",
   "metadata": {},
   "source": [
    "Data can be read from online sources, as in the example above where we just Loaded those into memory in parallel using Dask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b02010-b110-4212-ac0d-5ebea511bb0b",
   "metadata": {},
   "source": [
    "## Accessing Coordinates and Data Variables \n",
    "DataArray, within Datasets, can be accessed through:\n",
    "- the dot notation like Dataset.NameofVariable  \n",
    "- or using square brackets, like Dataset['NameofVariable'] (NameofVariable needs to be a string so use quotes or double quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ea8f6-3174-49dd-a41b-a0e1c1626280",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5069b6c-5ef1-4b5c-bfd1-516172134a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547cf63-0c7b-4b7e-83c7-43d9677f6129",
   "metadata": {},
   "source": [
    "ds['lai'] is a one-dimensional `xarray.DataArray` with dates of type `datetime64[ns]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e428ac-ef07-407d-a8cc-e4024f67ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['lai']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5835c07e-b422-48c5-b61b-ffbea0ccdba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['lai'].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173e45c-8668-47be-abd2-bc1b616737a8",
   "metadata": {},
   "source": [
    "### Xarray and Memory usage\n",
    "\n",
    "Once a Data Array|Set is opened, xarray loads into memory only the coordinates and all the metadata needed to describe it.\n",
    "The underlying data, the component written into the datastore, are loaded into memory as a NumPy array, only once directly accessed; once in there, it will be kept to avoid re-readings.\n",
    "This brings the fact that it is good practice to have a look to the size of the data before accessing it. A classical mistake is to try loading arrays bigger than the memory with the obvious result of killing a notebook Kernel or Python process.\n",
    "If the dataset does not fit in the available memory, then the only option will be to load it through the chunking; later on, in the tutorial 'chunking_introduction', we will introduce this concept.\n",
    "\n",
    "As the size of the data is not too big here, we can continue without any problem. But let's first have a look to the actual size and then how it impacts the memory once loaded into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ac66c-c7b3-4b95-bf97-bee4a6f8e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe0ec12-a485-4769-839a-a0585f6e31f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{np.round(ds.lai.nbytes / 1024**3, 2)} GB') # all the data are automatically loaded into memory as NumpyArray once they are accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53092fda-9a9a-4113-98ec-36ef6f831552",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lai.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3080383-f9eb-4cbc-ac66-3e850153e90d",
   "metadata": {},
   "source": [
    "As other datasets have dimensions named according to the more common triad lat,lon,time a renomination is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c84c42-c2f9-4738-a7b0-798cb58660ae",
   "metadata": {},
   "source": [
    "## Selection methods\n",
    "\n",
    "As underneath DataArrays are Numpy Array objects (that implement the standard Python x[obj] (x: array, obj: int,slice) syntax). Their data can be accessed through the same approach of numpy indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8a6f2-fc56-445d-9020-ad97dee381b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lai[0,100,100].load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227e300-9dcb-4dcc-b308-4e807784a808",
   "metadata": {},
   "source": [
    "As it is not easy to remember the order of dimensions, Xarray really helps by making it possible to select the position using names:\n",
    "\n",
    "- `.isel` -> selection based on positional index\n",
    "- `.sel`  -> selection based on coordinate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee07cfb9-15b2-45a9-bebc-b191f926362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lai.isel(time=0, latitude=100, longitude=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ae5076-5290-47c2-a467-37a19c046e62",
   "metadata": {},
   "source": [
    "The more common way to select a point is through the labeled coordinate using the `.sel` method.\n",
    "\n",
    "Time is easy to be used as there is a 1 to 1 correspondence with values in the index, float values are not that easy to be used and a small discrepancy can make a big difference in terms of results.\n",
    "\n",
    "\n",
    "Coordinates are always affected by precision issues; the best option to quickly get a point over the coordinates is to set the sampling method (method='') that will search for the closest point according to the specified one.\n",
    "\n",
    "Options for the method are:\n",
    "- pad / **f**fill: propagate last valid index value forward\n",
    "- backfill / **b**fill: propagate next valid index value backward\n",
    "- nearest: use nearest valid index value\n",
    "\n",
    "Another important parameter that can be set is the tolerance that specifies the distance between the requested and the target (so that abs(index\\[indexer] - target) <= tolerance) from [documentation](https://xarray.pydata.org/en/v0.17.0/generated/xarray.DataArray.sel.html#:~:text=xarray.DataArray.sel%20%C2%B6%20DataArray.sel%28indexers%3DNone%2C%20method%3DNone%2C%20tolerance%3DNone%2C%20drop%3DFalse%2C%20%2A%2Aindexers_kwargs%29%20%C2%B6,this%20method%20should%20use%20labels%20instead%20of%20integers.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3323028-b1db-41fd-b5d9-975ea9a20118",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lai.sel(time=datetime(2020, 1, 8), method='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df048d8e-23dd-409b-8bfd-5c1ef42d2b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lai.sel(latitude=46.3, longitude=8.8, method='nearest').isel(time=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4160ba5-94d0-4ef2-8cc6-0c41cd09c1c1",
   "metadata": {},
   "source": [
    ":::{warning}\n",
    "To select a single real value without specifying a method, you would need to specify the exact encoded value; not the one you see when printed.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78960dde-fb36-471e-af7f-67e05c0c2779",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lai.isel(longitude=100).longitude.values.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0aba3f-1585-4052-9678-7bab10400f9c",
   "metadata": {},
   "source": [
    "#### How does Xarray with Dask distribute data analysis?\n",
    "\n",
    "When we use chunks with `Xarray`, the real computation is only done when needed or asked for, usually when invoking `compute()` or `load()` functions. Dask generates a **task graph** describing the computations to be done. When using [Dask Distributed](https://distributed.dask.org/en/stable/) a **Scheduler** distributes these tasks across several **Workers**.\n",
    "\n",
    "![Xarray with dask](./static/dask-xarray-explained.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0029697b-b696-4611-af8c-baba0ee93611",
   "metadata": {},
   "source": [
    "#### How does Dask scale and accelerate your data analysis?\n",
    "\n",
    "[Dask proposes different abstractions to distribute your computation](https://docs.dask.org/en/stable/10-minutes-to-dask.html). In this _Dask Introduction_ section, we will focus on [Dask Array](https://docs.dask.org/en/stable/array.html) which is widely used in pangeo ecosystem as a back end of Xarray.\n",
    "\n",
    "As shown in the [previous section](./chunking_introduction.ipynb) Dask Array is based on chunks.\n",
    "Chunks of a Dask Array are well-known Numpy arrays. By transforming our big datasets to Dask Array, making use of chunk, a large array is handled as many smaller Numpy ones and we can compute each of these chunks independently.\n",
    "\n",
    "![Dask and Numpy](https://examples.dask.org/_images/dask-array-black-text.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3204c37-3d99-4a7e-9fd2-b1c9f310908a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e97a9ab8-ce0e-42ec-ad72-cd09cbbbf372",
   "metadata": {},
   "source": [
    "\n",
    "## Plotting\n",
    "   Plotting data can easily be obtained through matplotlib.pyplot back-end [matplotlib documentation](https://matplotlib.org/stable/index.html).\n",
    "\n",
    "As the exercise is focused on an Area Of Interest, this can be obtained through a bounding box defined with slices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8112605-cb0b-4e32-b6e6-9410be897b66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9981695b-fd4e-4180-ac0d-e8fcca22fdd3",
   "metadata": {},
   "source": [
    "Plot fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430d066-4843-4d3e-b236-0825c1eb9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "lai_aoi = ds.lai.sel(latitude=slice(65.5,25.5), longitude=slice(-20.5,20.5))\n",
    "lai_aoi.sel(time=datetime(2020,6,23), method='nearest').plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f175227-67c9-434d-97b2-cac9889a8fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae08e901-35f6-4288-9803-ed87f2e3a42b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e405179b-b538-4987-9063-453cd894c728",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "Have you noticed that latitudes are selected from the largest to the smallest values e.g. `46.5`, `44.5` while longitudes are selected from the smallest to the largest value e.g. `8.5`,`11.5`?\n",
    "**The reason is that you need to use the same order as the corresponding DataArray**.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317cea19-bf4c-48ff-aa8d-0a51e6c32237",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "Not all values are valid and masking all those which are not in the valid range is necessary. Masking can be achieved through the method `DataSet|Array.where(cond, other)` or `xr.where(cond, x, y)`.\n",
    "\n",
    "The difference consists in the possibility to specify the value in case the condition is positive or not; `DataSet|Array.where(cond, other)` only offer the possibility to define the false condition value (by default is set to np.NaN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb74710-21c6-44c9-be3d-eb47d9578efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "336667e2-52a5-4895-8850-840840562a60",
   "metadata": {},
   "source": [
    "# Basic Maths and Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ccf681-5371-40a7-b14e-66af4712f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E.g. example scaling \n",
    "\n",
    "lai_aoi * 0.0001 + 1500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd974fcb-b456-4af8-9b96-e7ab2ebb539f",
   "metadata": {},
   "source": [
    "# Statistics and Aggregation\n",
    "Calculate simple statistics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d69129-a756-4881-843e-6831dcff49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lai_aoi.min()\n",
    "lai_aoi.max()\n",
    "lai_aoi.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d3f78-88af-45f5-a16d-7ed688ca940b",
   "metadata": {},
   "source": [
    "Aggregate by month if the dataset spans multiple months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f9d33-fd2f-4e20-adc8-8095f922240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lai_monthly = lai_aoi.groupby(lai_aoi.time.dt.month).mean()\n",
    "lai_monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a3a275-e1b0-42f7-8232-c60fc60990d3",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "Not all values are valid and masking all those which are not in the valid range is necessary. Masking can be achieved through the method `DataSet|Array.where(cond, other)` or `xr.where(cond, x, y)`.\n",
    "\n",
    "The difference consists in the possibility to specify the value in case the condition is positive or not; `DataSet|Array.where(cond, other)` only offer the possibility to define the false condition value (by default is set to np.NaN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad6f81-5dfd-4a19-a698-711f583fedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lai_masked = lai_aoi.where((lai_aoi >= 1.5)) \n",
    "lai_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb52b5ad-02b0-40ac-a128-f1476213e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lai_masked.isel(time=0).plot(cmap='viridis')\n",
    "plt.title(\"Masked Leaf Area Index\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b98c8d7-f281-4591-b278-a595d18fc859",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = xr.where((lai_aoi > 1.5), 1, 0)\n",
    "mask.isel(time=0).plot()\n",
    "plt.title(\"Masked Leaf Area Index\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e174f0f-8e00-4383-8c76-40dbf7cf6b7b",
   "metadata": {},
   "source": [
    "By inspecting any of the variables on the representation above, you'll see that each data array represents __about 85GiB of data__, so much more than the availabe memory on this notebook server, and even on the Dask Cluster we created above. But thanks to chunking, we can still analyze it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4351041-57e1-4686-bd47-8a5919ef7373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e7ec634-5b59-4311-bb7f-bfe0a643e6ba",
   "metadata": {},
   "source": [
    "Did you notice something on the Dask Dashboard when running the two previous cells?\n",
    "\n",
    "We didn't 'compute' anything. We just built a Dask task graph with it's size indicated as count above, but did not ask Dask to return a result.\n",
    "\n",
    "But the 'task Count' we see above is more than 6000 for just computing a mean on 36 temporal steps. This is too much.  If you have such case, to avoid unecessary operations, you can optimize the task using `dask.optimize`. \n",
    "\n",
    "Lets try to plot the dask graph before computation and understand what dask workers will do to compute the value we asked for. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8051c6-d006-4752-84a7-a8d931b84291",
   "metadata": {},
   "source": [
    "Calling compute or triggering it (via plot) on our Xarray object triggered the execution on Dask Cluster side. \n",
    "\n",
    "You should be able to see how Dask is working on Dask Dashboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce1b58-c803-4170-85d4-2cd32ea60937",
   "metadata": {},
   "source": [
    "# Forest Fires Example Workflow\n",
    "\n",
    "Search for a forest fire in Europe this last year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca3434a-210d-465c-92b1-79008507d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.gfed_ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade082d-87d8-4c35-aac2-67b347dc67c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwis = ds.gwis_ba\n",
    "gwis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5ff45-a257-4a7f-be39-22d0079fe7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon = gpd.GeoDataFrame(index=[0], crs=\"epsg:4326\", geometry=[geometry.box(*bbox)])\n",
    "min_lon, min_lat, max_lon, max_lat = polygon.total_bounds\n",
    "\n",
    "offset = 0\n",
    "zoom = 1\n",
    "\n",
    "lat_start = gwis.latitude.sel(latitude=max_lat, method=\"nearest\").item()   + offset*zoom\n",
    "lat_stop  = gwis.latitude.sel(latitude=min_lat, method=\"nearest\").item()   - offset*zoom\n",
    "lon_start = gwis.longitude.sel(longitude=min_lon, method=\"nearest\").item()  - offset*zoom  # better orientation\n",
    "lon_stop  = gwis.longitude.sel(longitude=max_lon, method=\"nearest\").item()  + offset*zoom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc33b8-99ff-4c46-a2d2-d01eb8a16bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## in SeasFire Datacube v2.0 the burned area variables would have the water bodies masked with ERA-5 land sea mask \n",
    "mask= ds['lsm'][:,:]\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2782e3-163e-40a0-8179-8b59932da82c",
   "metadata": {},
   "source": [
    "Find the biggest forest fire during the last three years within that place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d33b7a-61a3-468a-9759-9101d627d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwis_aoi = gwis.sel(time=slice(datetime(2018,1,1),datetime(2021,1,1)), latitude=slice(lat_start, lat_stop),longitude=slice(lon_start, lon_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f71db-2fad-4be2-8e6a-5dafc48ee6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateind_max_fire = gwis_aoi.sum(dim={'latitude','longitude'}).argmax(dim='time').compute().item()\n",
    "fire_date = gwis_aoi.time[dateind_max_fire]\n",
    "# where the sum in the plot is the highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878ecc3c-103a-4b1f-a1df-3639df6bde70",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_max_fire = gwis_aoi.isel(time=dateind_max_fire)\n",
    "date_max_fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b95b98-5dcb-449c-bb80-d9abc4648ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gwis_all=gwis.resample(time=\"1Y\").sum()\n",
    "gwis_all=date_max_fire\n",
    "gwis_all= gwis_all.where(mask)\n",
    "gwis_all.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7711693-4072-43f8-b44a-84b22302d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cartopy\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cartopy.crs as ccrs\n",
    "\n",
    "\n",
    "\n",
    "# da = gwis_all\n",
    "\n",
    "# fig, ax = plt.subplots(\n",
    "#     figsize=(8, 6),\n",
    "#     subplot_kw={\"projection\": ccrs.PlateCarree()}\n",
    "# )\n",
    "\n",
    "# da.plot(\n",
    "#     ax=ax,\n",
    "#     transform=ccrs.PlateCarree(),\n",
    "#     cmap=\"viridis\",\n",
    "#     cbar_kwargs={\"label\": \"T₂ₘ (K)\"}\n",
    "# )\n",
    "\n",
    "# ax.coastlines(resolution=\"10m\", color=\"black\", linewidth=1)\n",
    "# ax.add_feature(cartopy.feature.BORDERS, linewidth=0.5)\n",
    "\n",
    "# ax.set_extent([lon_start, lon_stop, lat_stop, lat_start], crs=ccrs.PlateCarree())\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2578e-c9e7-4a8f-a06c-cf5cdd5656b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "\n",
    "# Plot it interactively\n",
    "gwis_all.hvplot(\n",
    "    x='longitude',\n",
    "    y='latitude',\n",
    "    cmap='viridis',\n",
    "    colorbar=True,\n",
    "    frame_width=600,\n",
    "    frame_height=400,\n",
    "    geo=True,\n",
    "    tiles='OSM'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb06b99-6846-4cf1-97e3-d0d5818f258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "\n",
    "# lat_start=41\n",
    "# lat_stop=41.5\n",
    "# lon_start=-7.7\n",
    "# lon_stop=-7.5\n",
    "\n",
    "# lat_slice=slice(41.5,41)\n",
    "# lon_slice=slice(-7.7,-7.5)\n",
    "\n",
    "# fires = gwis_all.sel(latitude=lat_slice,longitude=lon_slice)\n",
    "fire_date_t = pd.to_datetime(fire_date.values.item())\n",
    "\n",
    "week_before = (fire_date_t - timedelta(days=7))\n",
    "week_after = (fire_date_t + timedelta(days=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcea67e-ae5e-4764-bbdf-58541a72688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(week_before.date(), \"---\" , week_after.date())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db1ed9-11d1-4f9c-9bf4-d8fc6847e9fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac0f4f-2989-4e50-9a84-839e935a4a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pystac_client import Client as pystac_client\n",
    "# from odc.stac import configure_rio, stac_load\n",
    "\n",
    "\n",
    "# Open a catalog\n",
    "# catalog = pystac_client.open(\"https://earth-search.aws.element84.com/v1\")\n",
    "# time_start = \"2020-07-01\"\n",
    "# time_end = \"2020-07-28\"\n",
    "# time_range = time_start+\"/\"+time_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f37ae-d276-4d61-b403-1a6d58e29511",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c607a81-ba61-485b-9d7b-7dfa779b21a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'NBR'\n",
    "\n",
    "bandnames_dict = {\n",
    "    'nir': 'nir',\n",
    "    'swir22': 'swir22'\n",
    "}\n",
    "crs = \"epsg:\"+ str(epsg)  # projection on which the data will be projected\n",
    "\n",
    "# km2deg = 1.0 / 111\n",
    "# x, y = (23.9983519, 37.7351433)  # Center point of a query\n",
    "# r = 4 * km2deg  \n",
    "# bbox = (x - r, y - r, x + r, y + r)\n",
    "# zoom = 1\n",
    "\n",
    "\n",
    "# Normalised Burn Ratio, Lopez Garcia 1991\n",
    "def calc_nbr(ds):\n",
    "    return (ds.nir - ds.swir22) / (ds.nir + ds.swir22)\n",
    "\n",
    "index_dict = {'NBR': calc_nbr}\n",
    "index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e746ce-8b40-4299-b06e-173ff0743e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pystac_client.open(\"https://earth-search.aws.element84.com/v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e7083-ebb2-4022-967a-2541e1eeb53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom=1/2\n",
    "chunk={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08b52ea-30cb-41c1-9236-cf35f6e94f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_before_start = (week_before - timedelta(days=30))\n",
    "time_range = str(week_before_start.date()) + \"/\" + str(week_before.date())\n",
    "\n",
    "query1 = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"], datetime=time_range, limit=100,\n",
    "    bbox=bbox, query={\"eo:cloud_cover\": {\"lt\": 0.5}}\n",
    ")\n",
    "\n",
    "items = list(query1.get_items())\n",
    "print(f\"Found: {len(items):d} datasets\")\n",
    "\n",
    "items_pre = min(items, key=lambda item: item.properties[\"eo:cloud_cover\"])\n",
    "\n",
    "prefire_ds = stac_load(\n",
    "    [items_pre],\n",
    "    bands=(\"nir\", \"swir22\"),\n",
    "    # crs=crs,\n",
    "    # resolution=  10*zoom,\n",
    "    chunks=chunk,  # <-- use Dask\n",
    "    groupby=\"datetime\",\n",
    "    bbox=bbox,\n",
    ")\n",
    "prefire_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb6435-2d94-4cf2-b821-e438ba494702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7055b-5782-4af6-9c10-94819659ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_after_end = (week_after + timedelta(days=30))\n",
    "time_range = str(week_after.date()) + \"/\" + str(week_after_end.date())\n",
    "\n",
    "query2 = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"], datetime=time_range, limit=100,\n",
    "    bbox=bbox, query={\"eo:cloud_cover\": {\"lt\": 0.5}}\n",
    ")\n",
    "\n",
    "items = list(query2.get_items())\n",
    "print(f\"Found: {len(items):d} datasets\")\n",
    "\n",
    "items_post = min(items, key=lambda item: item.properties[\"eo:cloud_cover\"])\n",
    "\n",
    "postfire_ds = stac_load(\n",
    "    [items_post],\n",
    "    bands=(\"nir\", \"swir22\"),\n",
    "    # crs=crs,\n",
    "    # resolution=10 * zoom,\n",
    "    chunks=chunk,  # <-- use Dask\n",
    "    groupby=\"datetime\",\n",
    "    bbox=bbox,\n",
    ")\n",
    "postfire_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e71a9a-e330-4739-8fe0-5b4df1795b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4362690-e4ab-4074-9095-672edb85787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename bands in dataset to use simple names \n",
    "bands_to_rename = {\n",
    "    a: b for a, b in bandnames_dict.items() if a in prefire_ds.variables\n",
    "}\n",
    "\n",
    "# prefire\n",
    "prefire_ds[index_name] = index_dict[index_name](prefire_ds.rename(bands_to_rename) / 10000.0)\n",
    "\n",
    "# postfire\n",
    "postfire_ds[index_name] = index_dict[index_name](postfire_ds.rename(bands_to_rename) / 10000.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f594ea4-3091-48fd-9a70-8f26fdd94054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate delta NBR\n",
    "prefire_burnratio = prefire_ds.NBR.isel(time=0)\n",
    "postfire_burnratio = postfire_ds.NBR.isel(time=0)\n",
    "\n",
    "delta_NBR = prefire_burnratio - postfire_burnratio\n",
    "\n",
    "dnbr_dataset = delta_NBR.to_dataset(name='delta_NBR').persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e1aca-5885-4f86-8d50-951ed9b2182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnbr_dataset\n",
    "delta_NBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358844d-1afc-4d5d-aa68-eb07c39b029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=[7, 10])\n",
    "\n",
    "# We're using cartopy and are plotting in PlateCarree projection \n",
    "# (see documentation on cartopy)\n",
    "ax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "#ax.set_extent([-180, 180, -70, 70], crs=ccrs.PlateCarree()) # lon1 lon2 lat1 lat2\n",
    "ax.coastlines(resolution='10m')\n",
    "ax.gridlines(draw_labels=True)\n",
    "\n",
    "# We need to project our data to the new Orthographic projection and for this we use `transform`.\n",
    "# we set the original data projection in transform (here Mercator)\n",
    "prefire_burnratio.plot(ax=ax, transform=ccrs.epsg(prefire_burnratio.spatial_ref.values), cmap='RdBu_r',\n",
    "                       cbar_kwargs={'orientation':'horizontal','shrink':0.95})\n",
    "\n",
    "# One way to customize your title\n",
    "plt.title( pd.to_datetime(prefire_burnratio.time.values.item()).strftime(\"%d %B %Y\"), fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ddf95-672f-432f-b1b2-047b4f4a54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=[7, 9])\n",
    "\n",
    "# We're using cartopy and are plotting in PlateCarree projection \n",
    "# (see documentation on cartopy)\n",
    "ax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "#ax.set_extent([-180, 180, -70, 70], crs=ccrs.PlateCarree()) # lon1 lon2 lat1 lat2\n",
    "ax.coastlines(resolution='10m')\n",
    "ax.gridlines(draw_labels=True)\n",
    "\n",
    "# We need to project our data to the new Orthographic projection and for this we use `transform`.\n",
    "# we set the original data projection in transform (here Mercator)\n",
    "postfire_burnratio.plot(ax=ax, transform=ccrs.epsg(postfire_burnratio.spatial_ref.values), cmap='RdBu_r',\n",
    "                        cbar_kwargs={'orientation':'horizontal','shrink':0.95})\n",
    "\n",
    "# One way to customize your title\n",
    "plt.title( pd.to_datetime(postfire_burnratio.time.values.item()).strftime(\"%d %B %Y\"), fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14caa7b7-7026-4039-8da4-02a1394d40ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=[7, 10])\n",
    "\n",
    "# We're using cartopy and are plotting in PlateCarree projection \n",
    "# (see documentation on cartopy)\n",
    "ax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "ax.coastlines(resolution='10m')\n",
    "ax.gridlines(draw_labels=True)\n",
    "\n",
    "# We need to project our data to the new Orthographic projection and for this we use `transform`.\n",
    "# we set the original data projection in transform (here Mercator)\n",
    "dnbr_dataset.delta_NBR.plot(ax=ax, transform=ccrs.epsg(dnbr_dataset.delta_NBR.spatial_ref.values), cmap='RdBu_r',\n",
    "                            cbar_kwargs={'orientation':'horizontal','shrink':0.95})\n",
    "\n",
    "# One way to customize your title\n",
    "plt.title( \"Delta NBR\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ce2a3-d72c-4d98-886a-6765a7cde16b",
   "metadata": {},
   "source": [
    "https://un-spider.org/advisory-support/recommended-practices/recommended-practice-burn-severity/in-detail/normalized-burn-ratio\n",
    "\n",
    "![img](https://un-spider.org/sites/default/files/table+legend.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceebd22-b673-4bee-9489-d7f22904fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "BURN_THRESH = 0.27\n",
    "burn_mask = dnbr_dataset.delta_NBR > BURN_THRESH           # True/False mask, same shape as raster\n",
    "burn_mask.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e1af97-4521-471c-99fb-6e934e2a3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx, dy = dnbr_dataset.delta_NBR.rio.resolution()\n",
    "pixel_area_ha = abs(dx * dy) / 1e4       # 10m × 10m  → 0.01 ha\n",
    "pixel_area_ha\n",
    "\n",
    "pixels_burned   = burn_mask.sum().compute().item()   # integer number of burned pixels\n",
    "burned_area_ha  = pixels_burned * pixel_area_ha\n",
    "\n",
    "print(f\"Pixels burned : {pixels_burned:,d}\")\n",
    "print(f\"Burned area   : {burned_area_ha:,.2f} ha\")\n",
    "print(f\"Actual Burned Area : {gwis_all.sum().compute():,.2f}, ha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16bcf9a-71cf-46b0-8a3b-bf72b3e7d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnbr_dataset['burned_ha_mask'] = burn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d402ab-41cd-4dba-b5d2-39f5d949c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnbr_dataset = dnbr_dataset.persist()\n",
    "gwis_all = gwis_all.persist()\n",
    "\n",
    "\n",
    "# r_dnbr_dataset = dnbr_dataset.rename({'x': 'lon', 'y': 'lat'})\n",
    "# r_gwis_all = gwis_all.rename({'longitude': 'lon', 'latitude': 'lat'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e24d9f-d8a8-4879-9145-68327b0268bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwis_all = gwis_all.rio.write_crs(ds.rio.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda89645-3469-4c29-81de-f4d5a020cb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dea004b6-30dc-4ffa-a20b-32f1abd6deed",
   "metadata": {},
   "source": [
    "Plot is off because of bad projection (curcilinear to rectilinear) but we can see that generally the fires are in the north-west/north-east regions with two distinct occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f1959-938a-41d9-8673-7f030608e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "\n",
    "gwis_all_reprojected = gwis_all.rio.reproject(dnbr_dataset.delta_NBR.rio.crs)\n",
    "\n",
    "dnbr_plot = dnbr_dataset.delta_NBR.hvplot(\n",
    "    width=700,\n",
    "    height=700,\n",
    "    title='dNBR (10 m) with GWIS overlay',\n",
    "    alpha=1.0\n",
    ")\n",
    "\n",
    "# Plot the reprojected coarse dataset as transparent overlay\n",
    "gwis_plot = gwis_all_reprojected.hvplot(\n",
    "    cmap='Reds',\n",
    "    alpha=0.3,\n",
    "    clim=(0, gwis_all.max().compute().item())\n",
    ")\n",
    "\n",
    "# Combine them interactively\n",
    "combined_plot = dnbr_plot * gwis_plot\n",
    "\n",
    "combined_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2508b-51de-40b7-9268-55352aebb8b3",
   "metadata": {},
   "source": [
    "# Saving Your Work\n",
    "\n",
    "xrlint and validate your cube..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87fe44-6081-4600-8a49-59d6596596a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xrlint.all as xrl\n",
    "\n",
    "linter = xrl.new_linter(\"recommended\")\n",
    "linter.validate(dnbr_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27975471-0a3b-4a8d-ae9d-0cbe281fe265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign dataset-level attributes\n",
    "dnbr_dataset.attrs.update({\n",
    "    'title': 'Delta NBR and Burned Area Mask Dataset',\n",
    "    'history': 'Created by reprojecting and aligning datasets for fire severity analysis',\n",
    "    'Conventions': 'CF-1.7'\n",
    "})\n",
    "\n",
    "\n",
    "# Assign variable-level attributes for delta_NBR\n",
    "dnbr_dataset.delta_NBR.attrs.update({\n",
    "    'institution': 'Lampata',\n",
    "    'source': 'Sentinel-2 imagery; processed with open-source dNBR code, element84...',\n",
    "    'references': 'https://example.com/ref',\n",
    "    'comment': 'dNBR values represent change in vegetation severity post-fire',\n",
    "    'standard_name': 'difference_normalized_burn_ratio',\n",
    "    'long_name': 'Differenced Normalized Burn Ratio (dNBR)',\n",
    "    'units': 'm'\n",
    "})\n",
    "\n",
    "# Example for burned_ha_mask data variable\n",
    "dnbr_dataset.burned_ha_mask.attrs.update({\n",
    "    'standard_name': 'burned_area_mask',\n",
    "    'long_name': 'Burned Area Mask in Hectares',\n",
    "    'units': 'hectares',\n",
    "    'institution': 'Your Institution Name',\n",
    "    'source': 'Derived from wildfire impact analysis',\n",
    "    'references': 'https://example.com/ref',\n",
    "    'comment': 'Burned area mask showing presence of burned areas'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915b6804-6c2a-4e4f-b31e-110b34bea418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcube.core.verify import assert_cube\n",
    "\n",
    "assert_cube(dnbr_dataset)  # raises ValueError if it's not xcube-valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaafa64b-614f-4fa4-9e61-5e47a2804ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add time\n",
    "dnbr_dataset = dnbr_dataset.expand_dims(time=[postfire_ds.time.isel(time=0).values])\n",
    "dnbr_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6396c-f8f7-4500-9b75-1f6ea8b1b134",
   "metadata": {},
   "source": [
    "Chunk Data for Better Usability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2dd0c-4cca-4aad-8dc9-d1f6161670dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnbr_dataset = dnbr_dataset.chunk({\"time\": 1, \"y\": 1000, \"x\": 1000})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b937fa-a556-4d96-a09c-8a946a1a2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(dnbr_dataset.burned_ha_mask.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e699c-3b6b-480f-903a-7d957dfdd062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_at_folder = './wildfires'\n",
    "if not os.path.exists(save_at_folder):\n",
    "    os.makedirs(save_at_folder)\n",
    "\n",
    "# Define the output path within your notebook folder\n",
    "output_path = os.path.join(save_at_folder, \"dnbr_dataset.zarr\")\n",
    "\n",
    "# save\n",
    "dnbr_dataset.to_zarr(output_path, mode=\"w\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo] *",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
