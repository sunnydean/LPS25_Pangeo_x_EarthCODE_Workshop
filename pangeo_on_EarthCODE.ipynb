{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395c42f3-5ca6-4e54-9a25-a1dfbca2df6b",
   "metadata": {},
   "source": [
    "---\n",
    "title: Pangeo X EarthCODE\n",
    "subtitle: D.03.10 HANDS-ON TRAINING - EarthCODE 101 Hands-On Workshop - Example showing how to access data on the EarthCODE Open Science Catalog and working with the Pangeo ecosystem on EDC\n",
    "authors:\n",
    "  - name: Deyan Samardzhiev\n",
    "    github: sunnydean\n",
    "    orcid: 0009-0003-3803-8522\n",
    "    affiliations:\n",
    "      - id: Lampata UK\n",
    "        institution: Lampata UK\n",
    "reviewers:\n",
    "  - name: Anne Fouilloux\n",
    "    orcid: 0000-0002-1784-2920\n",
    "    github: annefou\n",
    "    affiliations:\n",
    "      - id: Simula Research Laboratory\n",
    "        institution: Simula Research Laboratory\n",
    "        ror: 00vn06n10\n",
    "date: 2025-06-01\n",
    "thumbnail: https://raw.githubusercontent.com/ESA-EarthCODE/documentation/refs/heads/main/pages/public/img/EarthCODE_kv_transparent.png\n",
    "keywords: [\"earthcode\", \"pangeo\", \"stac\", \"xarray\", \"earth observation\", \"remote sensing\"]\n",
    "tags: [\"pangeo\"]\n",
    "releaseDate: 2025-06-01\n",
    "datePublished: 2025-06-01\n",
    "dateModified: 2025-06-01\n",
    "banner: ./static/PANGEO.png\n",
    "github: https://github.com/sunnydean/LPS25_Pangeo_x_EarthCODE_Workshop\n",
    "license: MIT\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3644266a-6137-4806-9070-5ace09114025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove for EDC actual version, just for local testing\n",
    "from distributed.client import _global_clients\n",
    "for client in list(_global_clients.values()):\n",
    "    client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22680c10-8e98-4cc9-a091-f54c02a3e6e3",
   "metadata": {},
   "source": [
    "This hands-on workshop is designed to introduce participants to EarthCODE's capabilities, guiding them from searching, finding, and accessing EO datasets and workflows to publishing reproducible experiments that can be shared with the wider scientific community. This workshop will equip you with the tools and knowledge to leverage EarthCODE for your own projects and contribute to the future of open science. During this 90 minute workshop, participants will, in a hands-on fashion, learn about: - Introduction to EarthCODE and the future of FAIR and Open Science in Earth Observation - Gain understanding in Finding, Accessing, Interoperability, and Reusability of data and workflows on EarthCODE - Creating reproducible experiments using EarthCODEâ€™s platforms - with a hands-on example with Euro Data Cube and Pangeo - Publishing data and experiments to EarthCODE At the end of the workshop, we will take time for discussion and feedback on how to make EarthCODE better for the community.\n",
    "\n",
    "**Pre-requirements for attendees**: The participants need to bring their laptop and have an active github account but do not need to install anything as the resources will be accessed online using Pangeo notebooks provided by EarthCODE and EDC.\n",
    "\n",
    "Please register your interest by filling in this form: https://forms.office.com/e/jAB9YLjgY0 before the session.\n",
    "\n",
    ":::{hint} Learning Objectives\n",
    "- Get familiar with the EDC Pangeo Cloud Platform and Dask.\n",
    "- Access EarthCODE Open Science Catalog using the STAC API.\n",
    "- Understand and work with Zarr-formatted data and chunking.\n",
    "- Use Xarray and Dask for scalable geospatial analysis.\n",
    "- Run a complete data analysis example using EarthCODE resources.\n",
    "- Save and publish results to the EarthCODE Catalog.\n",
    ":::\n",
    "\n",
    "\n",
    "## Table of Content\n",
    "```{contents}\n",
    ":depth: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a9bf3-233b-41de-b614-14703b47429f",
   "metadata": {},
   "source": [
    "# About the Environment: Introducing the EDC Pangeo Cloud Platform\n",
    "## Pangeo on EOxHub\n",
    "\n",
    "You can now work with the Pangeo stack on the EDC EOxHub!\n",
    "\n",
    "**What is Pangeo?**\n",
    "\n",
    "Pangeo is a community-driven open-source ecosystem designed for scalable, Pythonic big data analysis. It brings a suite of powerful libraries that make working with large Earth observation and climate datasets easier and more interactive.\n",
    "\n",
    "**Key Tools in Pangeo** that you will learn about today:\n",
    "- **Xarray**: Handles multi-dimensional labeled arrays (like NetCDF), with intuitive syntax that feels natural for Python users.\n",
    "- **STAC**: Simplifies discovery and access to large Earth observation datasets with a consistent, JSON-based catalog standard.\n",
    "- **Dask**: Enables parallel and distributed computing, scaling analysis from laptops to powerful cloud clusters.\n",
    "- **Zarr**: Provides efficient, cloud-native storage for chunked and compressed data, perfect for massive datasets.\n",
    "- **Jupyter**: Delivers an interactive, shareable environment for building and sharing workflows.\n",
    "\n",
    "These tools work together embracing the Pythonic way of data science and enabling intuitive exploration and high-performance analysis in the cloud.\n",
    "\n",
    "\n",
    "**Euro Data Cube (EDC)**  \n",
    "A cloud platform for Earth observation data access and analysis, featuring a rich data catalog (Sentinel, Landsat, Copernicus, etc.), cloud-native analytics, and collaborative sharing tools for environmental, disaster, and climate applications. See more details at: [eurodatacube.com](https://eurodatacube.com)\n",
    "\n",
    "\n",
    "**EDC EOxHub Workspace Offering**  \n",
    "The EDC EOxHub Workspaces provide managed JupyterLab environments with curated images for EO projects. They offer customizable computing resources, persistent storage, and fast network connections. Users can run notebooks and applications, with fair use limits based on resource consumption. See more details at: [EOxHub Workspace](https://eurodatacube.com/marketplace/infra/edc_eoxhub_workspace)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6eee78-bd15-4789-80b1-184996264c65",
   "metadata": {},
   "source": [
    ":::{hint} Network of Resources Sponsorship!\n",
    "\n",
    "NoR provides non-commercial and commercial users with a unique environment to discover European cloud services and their estimate costs for Earth Observation exploitation. ESA offers sponsorship to eligible entities to cover the costs of trying out the various services.\n",
    "\n",
    "**You can apply to sponsor your project's compute and data access at: https://nor-discover.org/**\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6edfbd-b4c2-4f56-a315-8355e53e52ac",
   "metadata": {},
   "source": [
    "## Context\n",
    "We will be using the [Pangeo](https://pangeo.io/) open-source software stack to demonstrate how to fetch EarthCODE published data and publically available satellite Sentinel-2 data to generate burn severity maps for the assessment of the areas affected by wildfires.\n",
    "\n",
    "### Methodology approach\n",
    "* Access Sentinel-2 L2A cloud optimised dataset through STAC\n",
    "* Compute the Normalised Burn Ratio (NBR) index to highlight burned areas\n",
    "* Classify burn severity\n",
    "\n",
    "### Highlights\n",
    "* The NBR index uses near-infrared (NIR) and shortwave-infrared (SWIR) wavelengths.\n",
    "\n",
    "\n",
    "## Data\n",
    "We will use Sentinel-2 data accessed via [element84's STAC API](https://element84.com/earth-search/) endpoint and the [SeasFire Data Cube](https://opensciencedata.esa.int/products/seasfire-cube/collection) to find burned areas, inspect them in more detail and generate burn severity maps for the assessment of the areas affected by wildfires.\n",
    "\n",
    "\n",
    "\n",
    "#### Related publications\n",
    "* https://www.sciencedirect.com/science/article/pii/S1470160X22004708#f0035\n",
    "* https://github.com/yobimania/dea-notebooks/blob/e0ca59f437395f7c9becca74badcf8c49da6ee90/Fire%20Analysis%20Compiled%20Scripts%20(Gadi)/dNBR_full.py\n",
    "* *Alonso, Lazaro, Gans, Fabian, Karasante, Ilektra, Ahuja, Akanksha, Prapas, Ioannis, Kondylatos, Spyros, Papoutsis, Ioannis, Panagiotou, Eleannna, Michail, Dimitrios, Cremer, Felix, Weber, Ulrich, & Carvalhais, Nuno. (2022). SeasFire Cube: A Global Dataset for Seasonal Fire Modeling in the Earth System (0.4) [Data set]. Zenodo. @alonso-2024. The same dataset can also be downloaded from Zenodo: https://zenodo.org/records/13834057*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e8c480-574f-4799-92b1-b95ef7ec040b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Running on EDC EOxHub Workspace\n",
    "\n",
    "If you are using the EDC environment and have accessed this example through the Open Science Catalog, this example should have the correct kernel selected, and packages installed. You can directly use this example as is, with Dask Gateway. You can access this example on EDC during the week of LPS and the week after if you have registered for this workshop!\n",
    "\n",
    "To sign-in use your github account.\n",
    "\n",
    "### Running on your own computer\n",
    "\n",
    "Most parts of this tutorial were designed to run with limited computer resources, so it is possible to run on your laptop.\n",
    "It is a bit more complicated as you will have to install the software environment yourself. Also you will not be able to test real cloud distributed processing with Dask gateway.\n",
    "\n",
    "Steps to run this tutorial on your own computer are listed below and demonstrated _through Linux commands only_:\n",
    "\n",
    "1. git clone the LPS-25 repository.\n",
    "```bash\n",
    "git clone https://github.com/sunnydean/LPS25_Pangeo_x_EarthCODE_Workshop.git\n",
    "```\n",
    "2. Install the required software environment with Conda. If you do not have Conda, install it by following these instructions (see [here](https://docs.conda.io/en/latest/miniconda.html)). Then create the environment, this can take a few minutes.\n",
    "```bash\n",
    "conda env create -n pangeo -f LPS25_Pangeo_x_EarthCODE_Workshop/environment.yml\n",
    "```\n",
    "3. Launch a Jupyterlab notebook server from this environment.\n",
    "```bash\n",
    "conda activate pangeo\n",
    "jupyter lab\n",
    "```\n",
    "4. Open a web browser and connect to the Jupyterlab provided URL (you should see it in the jupyter lab command outputs), something like: http://localhost:8888/lab?token=42fac6733c6854578b981bca3abf5152.\n",
    "5. Navigate to pangeo_on_EarthCODE using the file browser on the left side of the Jupyterlab screen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11295d7-5aed-48bb-bacc-b493b6d990e8",
   "metadata": {},
   "source": [
    "### Packages\n",
    "\n",
    "As best practices dictate, we recommend that you install and import all the necessary libraries at the top of your Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22168f3-4e2e-4835-85b0-98fdb2d96d28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import stackstac\n",
    "import xarray\n",
    "import pystac\n",
    "import dask.distributed\n",
    "import rasterio\n",
    "from odc.stac import stac_load\n",
    "import odc\n",
    "import os\n",
    "from odc.stac import configure_rio\n",
    "from pystac.extensions.datacube import DatacubeExtension\n",
    "from odc.stac import configure_s3_access, stac_load\n",
    "from pystac.extensions.storage import StorageExtension\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import xarray as xr\n",
    "from ipyleaflet import Map, Polygon\n",
    "from shapely import geometry\n",
    "from shapely.geometry import shape, box\n",
    "from ipyleaflet import Map, GeoJSON\n",
    "\n",
    "from pystac_client import Client as pystac_client\n",
    "from odc.stac import configure_rio, stac_load\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d5a734-efd2-41e2-a805-1c4cb667441e",
   "metadata": {},
   "source": [
    "### Plotting our downloaded polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5273b-3793-46be-a105-d62e3876bf3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the feature from a file\n",
    "with open(\"feature.json\") as f:\n",
    "    feature = json.load(f)\n",
    "\n",
    "poly = shape(feature[\"geometry\"])\n",
    "bbox = list(poly.bounds)\n",
    "center = ((bbox[1] + bbox[3]) / 2.0, (bbox[0] + bbox[2]) / 2.0)\n",
    "m = Map(center=center, zoom=10)\n",
    "epsg = 4326\n",
    "\n",
    "# Create polygon from lists of points\n",
    "polygon = Polygon(\n",
    "    locations=[\n",
    "        (bbox[1], bbox[0]),\n",
    "        (bbox[3], bbox[0]),\n",
    "        (bbox[3], bbox[2]),\n",
    "        (bbox[1], bbox[2]),\n",
    "    ],\n",
    "    color=\"green\",\n",
    "    fill_color=\"green\",\n",
    ")\n",
    "\n",
    "# Add the polygon to the map\n",
    "m.add(polygon)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf242d8-b3dd-4bde-8980-7bc6d35ffa4f",
   "metadata": {},
   "source": [
    "# Accessing the EarthCODE Catalog\n",
    "\n",
    "This section introduces [STAC](https://stacspec.org/), the SpatioTemporal Asset Catalog. STAC provides a standardized way to structure metadata about spatialotemporal data. The STAC community are building APIs and tools on top of this structure to make working with spatiotemporal data easier.\n",
    "\n",
    "\n",
    "Users of STAC will interact most often with Collections and Items (there's also Catalogs, which group together collections). A Collection is just a collection of items, plus some additional metadata like the license and summaries of what's available on each item. You can view available collections on the EarthCODE catalog with\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e743c-5edb-4409-97b7-1cc189510874",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = pystac_client.open(\"https://catalog.osc.earthcode.eox.at/\")\n",
    "cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5956604-0fa6-4065-8e00-032dc0203b42",
   "metadata": {},
   "source": [
    "Now let's search and access the same data we were just looking at, SeasFire https://opensciencedata.esa.int/products/seasfire-cube/collection\n",
    "\n",
    "\n",
    "In the examples we've seen so far, we've just been given a STAC item. How do you find the items you want in the first place? That's where a STAC API comes in.\n",
    "\n",
    "A STAC API is some web service that accepts queries and returns STAC objects. The ability to handle queries is what differentiates a STAC API from a static STAC catalog, where items are just present on some file system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521fad68-4608-4b2c-a02a-25ef5dfd03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO SEARCH AND ACCESS SEASFIRE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56f1e3e2-a968-4660-a9c7-c86efba57dec",
   "metadata": {},
   "source": [
    "The collection points to another collection, which contains the actual data. The EarthCODE STAC extension describes some metadata that enrich the STAC collection https://github.com/stac-extensions/osc.\n",
    "\n",
    "![img.png](static/EarthCODE-STAC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f183c8d-46e4-49f5-932e-af56234579c8",
   "metadata": {},
   "source": [
    "# Accessing Data from EarthCODE\n",
    "\n",
    "Now we will load the actual data from the STAC item we've found...\n",
    "\n",
    ">The SeasFire Cube is a scientific datacube for seasonal fire forecasting around the globe. It has been created for the SeasFire project, that adresses 'Earth System Deep Learning for Seasonal Fire Forecasting' and is funded by the European Space Agency (ESA)  in the context of ESA Future EO-1 Science for Society Call. It contains almost 20 years of data (2001-2021) in an 8-days time resolution and 0.25 degrees grid resolution. It has a diverse range of seasonal fire drivers. It expands from atmospheric and climatological ones to vegetation variables, socioeconomic and the target variables related to wildfires such as burned areas, fire radiative power, and wildfire-related CO2 emissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7c6bc-63d4-4419-a9d2-771d1b2d2323",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasfire = pystac.read_file(\n",
    "    \"https://s3.waw4-1.cloudferro.com/EarthCODE/Catalogs/seasfire/seasfire-cube_v0.4/catalog.json\"\n",
    ")\n",
    "\n",
    "for item in seasfire.get_items():\n",
    "    print(item)\n",
    "\n",
    "seasfire\n",
    "# https://s3.waw4-1.cloudferro.com/EarthCODE/Catalogs/seasfire/seasfire-cube_v0.4/seasfire-cube-v.0.4/seasfire-cube-v.0.4.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a0d49-7dc8-4b38-b90f-513d787a0fbe",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "When dealing with large data files or collections, it's often impossible to load all the data you want to analyze into a single computer's RAM at once. This is a situation where the Pangeo ecosystem can help you a lot. Xarray offers the possibility to work lazily on data __chunks__, which means pieces of an entire dataset. By reading a dataset in __chunks__ we can process our data piece by piece on a single computer and even on a distributed computing cluster using Dask (Cloud or HPC for instance).\n",
    "\n",
    "How we will process these 'chunks' in a parallel environment will be discussed in [dask_introduction](./dask_introduction.ipynb). The concept of __chunk__ will be explained here.\n",
    "\n",
    "When we process our data piece by piece, it's easier to have our input or ouput data also saved in __chunks__. [Zarr](https://zarr.readthedocs.io/en/stable/) is the reference library in the Pangeo ecosystem to save our Xarray multidimentional datasets in __chunks__.\n",
    "\n",
    "[Zarr](https://zarr.readthedocs.io/en/stable/) is not the only file format which uses __chunk__. We will also be using [kerchunk library](https://fsspec.github.io/kerchunk/) in this notebook to build a virtual __chunked__ dataset based on NetCDF files, and show how it optimizes the access and analysis of large datasets.\n",
    "\n",
    "The analysis is very similar to what we have done in previous episodes, however we will use data on a global coverage and not only on a small geographical area (e.g. Lombardia).\n",
    "\n",
    "### Data\n",
    "\n",
    "In this episode, we will be using Global Long Term Statistics (1999-2019) products provided by the [Copernicus Global Land Service](https://land.copernicus.eu/global/index.html) and access them through [S3-comptabile storage](https://en.wikipedia.org/wiki/Amazon_S3) ([OpenStack Object Storage \"Swift\"](https://wiki.openstack.org/wiki/Swift)) with a data catalog we have created and made publicly available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9069e4-6606-4457-8c5b-704b5b941d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasfire_cube = seasfire.get_item(\"seasfire-cube-v.0.4\")\n",
    "seasfire_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ce2c9-f02a-44a1-80a3-02e71dc2d9ee",
   "metadata": {},
   "source": [
    "## Assets\n",
    "STAC is a metadata standard. It doesn't really deal with data files directly. Instead, it links to the data files under the \"assets\" property.\n",
    "\n",
    "---\n",
    "\n",
    "The STAC catalog contains a collection for each Zarr store and there are collection-level assets that point to the location of the Zarr store. There are no items at all in this setup.\n",
    "\n",
    "In this scenario any STAC metadata exists purely for discovery and cannot be used for filtering or subsetting (see Future Work for more on that). To search the STAC catalog to find collections of interest you will use the Collection Search API Extension. Depending on the level of metadata that has been provided in the STAC catalog you can search by the name of the collection and possibly by the variables â€“ exposed via the Data Cube Extension.\n",
    "\n",
    "Read straight to xarray\n",
    "Once you have found the collection of interest, the best approach for accessing the data is to construct the lazily-loaded data cube in xarray (or an xarray.DataTree if the Zarr store has more than one group) and filter from there.\n",
    "\n",
    "To do this you can use the zarr backend directly or you can use the stac backend to streamline even more. The stac backend is mostly useful if the STAC collection uses the xarray extension.\n",
    "\n",
    "Constructing the lazy data cube is likely to be very fast if there is a consolidated metadata file OR the data is in Zarr-3 and the Zarr metadata fetch is highly parallelized (read more).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c2630-2155-42dc-b21f-13b1440598e4",
   "metadata": {},
   "source": [
    "## What is a __chunk__\n",
    "\n",
    "If you look carefully to `LTS`, each Data Variable is a `dask.array` with a chunk size of `(15680, 40320)`. So basically accessing one data variable would load arrays of dimensions `(15680, 40320)` into the computer's RAM. You can see this information and more details by clicking the icon as indicated in the image below.\n",
    "\n",
    "![Dask.array](./static/datasize.png)\n",
    "\n",
    "When you open one or several netCDF files with `open_mdfataset`, by default, the chunks correspond to the entire size of the variable data array read from each file. When you need to analyze large files, a computer's memory may not be sufficient anymore (see in this example, 2.36GiB for one chunk!).\n",
    "\n",
    "This is where understanding and using chunking correctly comes into play.\n",
    "\n",
    "\n",
    "__Chunking__ is splitting a dataset into small pieces. \n",
    "\n",
    "Original dataset is in one piece,  \n",
    "![Dask.array](./static/notchunked.png)\n",
    "\n",
    "and we split it into several smaller pieces.  \n",
    "![Dask.array](./static/chunked.png)\n",
    "\n",
    "We split it into pieces so that we can process our data block by block or __chunk__ by __chunk__.\n",
    "\n",
    "In our case, for the moment, the dataset is composed of several files, so already several pieces (or just one in the example above), and Xarray just creates one chunk for each variable of each file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317630c4-e066-494e-9cf5-a2dabaf197e1",
   "metadata": {},
   "source": [
    "Zarr format main characteristics are the following:\n",
    "\n",
    "- Every chunk of a Zarr dataset is stored as a single file (see x.y files in `ls -al test.zarr/nobs`)\n",
    "- Each Data array in a Zarr dataset has a two unique files containing metadata:\n",
    "  - .zattrs for dataset or dataarray general metadatas\n",
    "  - .zarray indicating how the dataarray is chunked, and where to find them on disk or other storage.\n",
    "  \n",
    "Zarr can be considered as an Analysis Ready, cloud optimized data (ARCO) file format, discussed in [data_discovery](./data_discovery.ipynb) section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2cf45-f9d6-406f-8fbd-32900265af08",
   "metadata": {},
   "source": [
    "### What is [Dask](https://docs.dask.org/) ?\n",
    "\n",
    "**Dask** scales the existing Python ecosystem: with very or no changes in your code, you can speed-up computation using Dask or process bigger than memory datasets.\n",
    "\n",
    "- Dask is a flexible library for parallel computing in Python.\n",
    "- It is widely used for handling large and complex Earth Science datasets and speed up science.\n",
    "- Dask is powerful, scalable and flexible. It is the leading platform today for data analytics at scale.\n",
    "- It scales natively to clusters, cloud, HPC and bridges prototyping up to production.\n",
    "- The strength of Dask is that is scales and accelerates the existing Python ecosystem e.g. Numpy, Pandas and Scikit-learn with few effort from end-users.\n",
    "\n",
    "It is interesting to note that at first, [Dask has been created to handle data that is larger than memory, on a single computer](https://coiled.io/blog/history-dask/). It then was extended with Distributed to compute data in parallel over clusters of computers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b165b8f-b690-41dd-ae39-89d3e72617ee",
   "metadata": {},
   "source": [
    "### What is a Dask Distributed cluster ?\n",
    "\n",
    "A Dask Distributed cluster is made of two main components:\n",
    "\n",
    "- a Scheduler, responsible for handling computations graph and distributing tasks to Workers.\n",
    "- One or several (up to 1000s) Workers, computing individual tasks and storing results and data into distributed memory (RAM and/or worker's local disk).\n",
    "\n",
    "A user usually needs __Client__ and __Cluster__ objects as shown below to use Dask Distributed.    \n",
    "\n",
    "![Dask Distributed Cluster](https://user-images.githubusercontent.com/306380/66413985-27111600-e9be-11e9-9995-8f418ff48f8a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d4cdd-2f6f-4033-a612-20262685def5",
   "metadata": {},
   "source": [
    "#### Where can we deploy a Dask distributed cluster?\n",
    "\n",
    "\n",
    "[Dask distributed clusters can be deployed on your laptop or on distributed infrastructures (Cloud, HPC centers, Hadoop, etc.).] - (https://docs.dask.org/en/stable/deploying.html)  Dask distributed `Cluster` object is responsible of deploying and scaling a Dask Cluster on the underlying resources.\n",
    "\n",
    "EDC has one such deployment\n",
    "\n",
    "\n",
    "![Dask Cluster deployment](https://docs.dask.org/en/stable/_images/dask-cluster-manager.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d39aa-7ff1-4b97-9f51-9ff72efc8a85",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "A Dask `Cluster` can be created on a single machine (for instance your laptop) e.g. there is no need to have dedicated computational resources. However, speedup will only be limited to your single machine resources if you do not have dedicated computational resources!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8b5dcd-e6dc-4c02-bd43-0023f1e4ee8b",
   "metadata": {},
   "source": [
    "## Scaling your Computation using Dask Gateway.\n",
    "\n",
    "For this workshop, according to the Pangeo EDC deployment, you will learn how to use Dask Gateway to manage Dask clusters over Kubernetes, allowing to run our data analysis in parallel e.g. distribute tasks across several workers.\n",
    "\n",
    "Lets set up your Dask cluster through Dask Gateway.  \n",
    "As Dask Gateway is configured by default on this ifnrastructure, you just need to execute the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b9f5a-e252-43b8-8740-73d162b13ece",
   "metadata": {},
   "source": [
    "Change to EDC Cluster instance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ff714-c514-40c7-b392-d243c9dc2d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92534dd5-64ab-4413-aabb-6d880db640ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21ad6c9a-2432-42bc-aedd-6059d65231a4",
   "metadata": {},
   "source": [
    "Inspecting the `Cluster Info` section above gives us information about the created cluster: we have 2 or 4 workers and the same number of threads (e.g. 1 thread per worker). \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Go further</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li> You can also create a local cluster with the `LocalCluster` constructor and use `n_workers` \n",
    "        and `threads_per_worker` to manually specify the number of processes and threads you want to use. \n",
    "        For instance, we could use `n_workers=2` and `threads_per_worker=2`.  </li>\n",
    "        <li> This is sometimes preferable (in terms of performance), or when you run this tutorial on your PC, \n",
    "        you can avoid dask to use all your resources you have on your PC!  </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0581ff1-c059-49f9-8f49-f700974bc4fe",
   "metadata": {},
   "source": [
    "### Dask distributed Client\n",
    " \n",
    "The Dask distributed `Client` is what allows you to interact with Dask distributed Clusters. When using Dask distributed, you always need to create a `Client` object. Once a `Client` has been created, it will be used by default by each call to a Dask API, even if you do not explicitly use it.\n",
    "\n",
    "No matter the Dask API (e.g. Arrays, Dataframes, Delayed, Futures, etc.) that you use, under the hood, Dask will create a Directed Acyclic Graph (DAG) of tasks by analysing the code. Client will be responsible to submit this DAG to the Scheduler along with the final result you want to compute. The Client will also gather results from the Workers, and aggregate it back in its underlying Python process.\n",
    "\n",
    "Using `Client()` function with no argument, you will create a local Dask cluster with a number of workers and threads per worker corresponding to the number of cores in the 'local' machine. Here, during the workshop, we are running this notebook in Pangeo EOSC cloud deployment, so the 'local' machine is the jupyterlab you are using at the Cloud, and the number of cores is the number of cores on the cloud computing resources you've been given (not on your laptop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6054176-3bba-474e-9b9f-cf4d89197a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78f0a8f5-8b22-4f75-9c8d-957f8a58c5db",
   "metadata": {},
   "source": [
    "### Dask Dashboard\n",
    "\n",
    "Dask comes with a really handy interface: the Dask Dashboard. It is a web interface that you can open in a separated tab of your browser (but not with he link above, you've got to use Jupyterlabs proxy: https://pangeo-foss4g.vm.fedcloud.eu/jupyterhub/user/_yourusername_/proxy/8787/status).\n",
    "\n",
    "We will learn here how to use it through [dask jupyterlab extension](https://github.com/dask/dask-labextension).  \n",
    "\n",
    "To use Dask Dashboard through jupyterlab extension on Pangeo EDC infrastructure,\n",
    "you will just need too look at the html link you have for your jupyterlab, and Dask dashboard port number, as highlighted in the figure below.\n",
    "\n",
    "![Dash Board link](./static/dashboardlink.png)\n",
    "![Dash lab](./static/dasklab.png)\n",
    "\n",
    "Then click the orange icon indicated in the above figure, and type 'your' dashboard link (normally, you just need to replace 'todaka' to 'your username').  \n",
    "\n",
    "\n",
    "You can click several buttons indicated with blue arrows in above figures, then drag and drop to place them as your convenience.  \n",
    "\n",
    "![Example dask lab](./static/exampledasklab.png)\n",
    "\n",
    "\n",
    "It's really helpfull to understand your computation and how it is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c27381-043a-4343-831e-95ae689cf5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "http_url = seasfire_cube.assets[\"data\"].href.replace(\n",
    "    \"s3://\",\n",
    "    f\"{seasfire_cube.properties['storage:schemes'][seasfire_cube.assets['data'].extra_fields['storage:refs'][0]]['platform'].rstrip('/')}/\",\n",
    ")\n",
    "\n",
    "ds = xarray.open_dataset(\n",
    "\thttp_url,\n",
    "\tengine='zarr',\n",
    "    chunks={},\n",
    "\tconsolidated=True\n",
    "\t# storage_options = {'token': 'anon'}\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343eb7b4-280e-478f-b301-2332102a9331",
   "metadata": {},
   "source": [
    "## What is xarray?\n",
    "\n",
    "Xarray introduces labels in the form of dimensions, coordinates and attributes on top of raw NumPy-like multi-dimensional arrays, which allows for a more intuitive, more concise, and less error-prone developer experience.\n",
    "\n",
    "### How is xarray structured?\n",
    "\n",
    "Xarray has two core data structures, which build upon and extend the core strengths of NumPy and Pandas libraries. Both data structures are fundamentally N-dimensional:\n",
    "\n",
    "- [DataArray](https://docs.xarray.dev/en/stable/generated/xarray.DataArray.html#xarray.DataArray) is the implementation of a labeled, N-dimensional array. It is an N-D generalization of a Pandas.Series. The name DataArray itself is borrowed from [Fernando Perezâ€™s datarray project](http://fperez.org/py4science/datarray/), which prototyped a similar data structure.\n",
    "\n",
    "- [Dataset](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.html#xarray.Dataset) is a multi-dimensional, in-memory array database. It is a dict-like container of DataArray objects aligned along any number of shared dimensions, and serves a similar purpose in xarray as the pandas.DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085af36-d7a5-4c73-9d35-525eef0d1ae4",
   "metadata": {},
   "source": [
    "Data can be read from online sources, as in the example above where we just Loaded those into memory in parallel using Dask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b02010-b110-4212-ac0d-5ebea511bb0b",
   "metadata": {},
   "source": [
    "## Accessing Coordinates and Data Variables \n",
    "DataArray, within Datasets, can be accessed through:\n",
    "- the dot notation like Dataset.NameofVariable  \n",
    "- or using square brackets, like Dataset['NameofVariable'] (NameofVariable needs to be a string so use quotes or double quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ea8f6-3174-49dd-a41b-a0e1c1626280",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5069b6c-5ef1-4b5c-bfd1-516172134a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547cf63-0c7b-4b7e-83c7-43d9677f6129",
   "metadata": {},
   "source": [
    "ds['lai'] is a one-dimensional `xarray.DataArray` with dates of type `datetime64[ns]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e428ac-ef07-407d-a8cc-e4024f67ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['lai']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5835c07e-b422-48c5-b61b-ffbea0ccdba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['lai'].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173e45c-8668-47be-abd2-bc1b616737a8",
   "metadata": {},
   "source": [
    "### Xarray and Memory usage\n",
    "\n",
    "Once a Data Array|Set is opened, xarray loads into memory only the coordinates and all the metadata needed to describe it.\n",
    "The underlying data, the component written into the datastore, are loaded into memory as a NumPy array, only once directly accessed; once in there, it will be kept to avoid re-readings.\n",
    "This brings the fact that it is good practice to have a look to the size of the data before accessing it. A classical mistake is to try loading arrays bigger than the memory with the obvious result of killing a notebook Kernel or Python process.\n",
    "If the dataset does not fit in the available memory, then the only option will be to load it through the chunking; later on, in the tutorial 'chunking_introduction', we will introduce this concept.\n",
    "\n",
    "As the size of the data is not too big here, we can continue without any problem. But let's first have a look to the actual size and then how it impacts the memory once loaded into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ac66c-c7b3-4b95-bf97-bee4a6f8e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe0ec12-a485-4769-839a-a0585f6e31f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{np.round(ds.lai.nbytes / 1024**3, 2)} GB') # all the data are automatically loaded into memory as NumpyArray once they are accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53092fda-9a9a-4113-98ec-36ef6f831552",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lai.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3080383-f9eb-4cbc-ac66-3e850153e90d",
   "metadata": {},
   "source": [
    "As other datasets have dimensions named according to the more common triad lat,lon,time a renomination is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c84c42-c2f9-4738-a7b0-798cb58660ae",
   "metadata": {},
   "source": [
    "## Selection methods\n",
    "\n",
    "As underneath DataArrays are Numpy Array objects (that implement the standard Python x[obj] (x: array, obj: int,slice) syntax). Their data can be accessed through the same approach of numpy indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8a6f2-fc56-445d-9020-ad97dee381b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lai[0,100,100].load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227e300-9dcb-4dcc-b308-4e807784a808",
   "metadata": {},
   "source": [
    "As it is not easy to remember the order of dimensions, Xarray really helps by making it possible to select the position using names:\n",
    "\n",
    "- `.isel` -> selection based on positional index\n",
    "- `.sel`  -> selection based on coordinate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee07cfb9-15b2-45a9-bebc-b191f926362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lai.isel(time=0, latitude=100, longitude=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ae5076-5290-47c2-a467-37a19c046e62",
   "metadata": {},
   "source": [
    "The more common way to select a point is through the labeled coordinate using the `.sel` method.\n",
    "\n",
    "Time is easy to be used as there is a 1 to 1 correspondence with values in the index, float values are not that easy to be used and a small discrepancy can make a big difference in terms of results.\n",
    "\n",
    "\n",
    "Coordinates are always affected by precision issues; the best option to quickly get a point over the coordinates is to set the sampling method (method='') that will search for the closest point according to the specified one.\n",
    "\n",
    "Options for the method are:\n",
    "- pad / **f**fill: propagate last valid index value forward\n",
    "- backfill / **b**fill: propagate next valid index value backward\n",
    "- nearest: use nearest valid index value\n",
    "\n",
    "Another important parameter that can be set is the tolerance that specifies the distance between the requested and the target (so that abs(index\\[indexer] - target) <= tolerance) from [documentation](https://xarray.pydata.org/en/v0.17.0/generated/xarray.DataArray.sel.html#:~:text=xarray.DataArray.sel%20%C2%B6%20DataArray.sel%28indexers%3DNone%2C%20method%3DNone%2C%20tolerance%3DNone%2C%20drop%3DFalse%2C%20%2A%2Aindexers_kwargs%29%20%C2%B6,this%20method%20should%20use%20labels%20instead%20of%20integers.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3323028-b1db-41fd-b5d9-975ea9a20118",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lai.sel(time=datetime(2020, 1, 8), method='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df048d8e-23dd-409b-8bfd-5c1ef42d2b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lai.sel(latitude=46.3, longitude=8.8, method='nearest').isel(time=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4160ba5-94d0-4ef2-8cc6-0c41cd09c1c1",
   "metadata": {},
   "source": [
    ":::{warning}\n",
    "To select a single real value without specifying a method, you would need to specify the exact encoded value; not the one you see when printed.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78960dde-fb36-471e-af7f-67e05c0c2779",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lai.isel(longitude=100).longitude.values.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0aba3f-1585-4052-9678-7bab10400f9c",
   "metadata": {},
   "source": [
    "#### How does Xarray with Dask distribute data analysis?\n",
    "\n",
    "When we use chunks with `Xarray`, the real computation is only done when needed or asked for, usually when invoking `compute()` or `load()` functions. Dask generates a **task graph** describing the computations to be done. When using [Dask Distributed](https://distributed.dask.org/en/stable/) a **Scheduler** distributes these tasks across several **Workers**.\n",
    "\n",
    "![Xarray with dask](./static/dask-xarray-explained.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0029697b-b696-4611-af8c-baba0ee93611",
   "metadata": {},
   "source": [
    "#### How does Dask scale and accelerate your data analysis?\n",
    "\n",
    "[Dask proposes different abstractions to distribute your computation](https://docs.dask.org/en/stable/10-minutes-to-dask.html). In this _Dask Introduction_ section, we will focus on [Dask Array](https://docs.dask.org/en/stable/array.html) which is widely used in pangeo ecosystem as a back end of Xarray.\n",
    "\n",
    "As shown in the [previous section](./chunking_introduction.ipynb) Dask Array is based on chunks.\n",
    "Chunks of a Dask Array are well-known Numpy arrays. By transforming our big datasets to Dask Array, making use of chunk, a large array is handled as many smaller Numpy ones and we can compute each of these chunks independently.\n",
    "\n",
    "![Dask and Numpy](https://examples.dask.org/_images/dask-array-black-text.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3204c37-3d99-4a7e-9fd2-b1c9f310908a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e97a9ab8-ce0e-42ec-ad72-cd09cbbbf372",
   "metadata": {},
   "source": [
    "\n",
    "## Plotting\n",
    "   Plotting data can easily be obtained through matplotlib.pyplot back-end [matplotlib documentation](https://matplotlib.org/stable/index.html).\n",
    "\n",
    "As the exercise is focused on an Area Of Interest, this can be obtained through a bounding box defined with slices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8112605-cb0b-4e32-b6e6-9410be897b66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9981695b-fd4e-4180-ac0d-e8fcca22fdd3",
   "metadata": {},
   "source": [
    "Plot fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430d066-4843-4d3e-b236-0825c1eb9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "lai_aoi = ds.lai.sel(latitude=slice(65.5,25.5), longitude=slice(-20.5,20.5))\n",
    "lai_aoi.sel(time=datetime(2020,6,23), method='nearest').plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f175227-67c9-434d-97b2-cac9889a8fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae08e901-35f6-4288-9803-ed87f2e3a42b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e405179b-b538-4987-9063-453cd894c728",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "Have you noticed that latitudes are selected from the largest to the smallest values e.g. `46.5`, `44.5` while longitudes are selected from the smallest to the largest value e.g. `8.5`,`11.5`?\n",
    "**The reason is that you need to use the same order as the corresponding DataArray**.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317cea19-bf4c-48ff-aa8d-0a51e6c32237",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "Not all values are valid and masking all those which are not in the valid range is necessary. Masking can be achieved through the method `DataSet|Array.where(cond, other)` or `xr.where(cond, x, y)`.\n",
    "\n",
    "The difference consists in the possibility to specify the value in case the condition is positive or not; `DataSet|Array.where(cond, other)` only offer the possibility to define the false condition value (by default is set to np.NaN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb74710-21c6-44c9-be3d-eb47d9578efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "336667e2-52a5-4895-8850-840840562a60",
   "metadata": {},
   "source": [
    "# Basic Maths and Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ccf681-5371-40a7-b14e-66af4712f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E.g. example scaling \n",
    "\n",
    "lai_aoi * 0.0001 + 1500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd974fcb-b456-4af8-9b96-e7ab2ebb539f",
   "metadata": {},
   "source": [
    "# Statistics and Aggregation\n",
    "Calculate simple statistics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d69129-a756-4881-843e-6831dcff49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lai_aoi.min()\n",
    "lai_aoi.max()\n",
    "lai_aoi.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d3f78-88af-45f5-a16d-7ed688ca940b",
   "metadata": {},
   "source": [
    "Aggregate by month if the dataset spans multiple months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f9d33-fd2f-4e20-adc8-8095f922240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lai_monthly = lai_aoi.groupby(lai_aoi.time.dt.month).mean()\n",
    "lai_monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a3a275-e1b0-42f7-8232-c60fc60990d3",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "Not all values are valid and masking all those which are not in the valid range is necessary. Masking can be achieved through the method `DataSet|Array.where(cond, other)` or `xr.where(cond, x, y)`.\n",
    "\n",
    "The difference consists in the possibility to specify the value in case the condition is positive or not; `DataSet|Array.where(cond, other)` only offer the possibility to define the false condition value (by default is set to np.NaN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad6f81-5dfd-4a19-a698-711f583fedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lai_masked = lai_aoi.where((lai_aoi >= 1.5)) \n",
    "lai_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb52b5ad-02b0-40ac-a128-f1476213e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lai_masked.isel(time=0).plot(cmap='viridis')\n",
    "plt.title(\"Masked Leaf Area Index\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b98c8d7-f281-4591-b278-a595d18fc859",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = xr.where((lai_aoi > 1.5), 1, 0)\n",
    "mask.isel(time=0).plot()\n",
    "plt.title(\"Masked Leaf Area Index\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e174f0f-8e00-4383-8c76-40dbf7cf6b7b",
   "metadata": {},
   "source": [
    "By inspecting any of the variables on the representation above, you'll see that each data array represents __about 85GiB of data__, so much more than the availabe memory on this notebook server, and even on the Dask Cluster we created above. But thanks to chunking, we can still analyze it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4351041-57e1-4686-bd47-8a5919ef7373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e7ec634-5b59-4311-bb7f-bfe0a643e6ba",
   "metadata": {},
   "source": [
    "Did you notice something on the Dask Dashboard when running the two previous cells?\n",
    "\n",
    "We didn't 'compute' anything. We just built a Dask task graph with it's size indicated as count above, but did not ask Dask to return a result.\n",
    "\n",
    "But the 'task Count' we see above is more than 6000 for just computing a mean on 36 temporal steps. This is too much.  If you have such case, to avoid unecessary operations, you can optimize the task using `dask.optimize`. \n",
    "\n",
    "Lets try to plot the dask graph before computation and understand what dask workers will do to compute the value we asked for. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8051c6-d006-4752-84a7-a8d931b84291",
   "metadata": {},
   "source": [
    "Calling compute or triggering it (via plot) on our Xarray object triggered the execution on Dask Cluster side. \n",
    "\n",
    "You should be able to see how Dask is working on Dask Dashboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce1b58-c803-4170-85d4-2cd32ea60937",
   "metadata": {},
   "source": [
    "# Forest Fires Example Workflow\n",
    "\n",
    "Search for a forest fire in Europe this last year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca3434a-210d-465c-92b1-79008507d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.gfed_ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade082d-87d8-4c35-aac2-67b347dc67c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwis = ds.gwis_ba\n",
    "gwis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5ff45-a257-4a7f-be39-22d0079fe7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon = gpd.GeoDataFrame(index=[0], crs=\"epsg:4326\", geometry=[geometry.box(*bbox)])\n",
    "min_lon, min_lat, max_lon, max_lat = polygon.total_bounds\n",
    "\n",
    "offset = 0\n",
    "zoom = 1\n",
    "# explore .where instead \n",
    "\n",
    "lat_start = gwis.latitude.sel(latitude=max_lat, method=\"nearest\").item()   + offset*zoom\n",
    "lat_stop  = gwis.latitude.sel(latitude=min_lat, method=\"nearest\").item()   - offset*zoom\n",
    "lon_start = gwis.longitude.sel(longitude=min_lon, method=\"nearest\").item()  - offset*zoom  # better orientation\n",
    "lon_stop  = gwis.longitude.sel(longitude=max_lon, method=\"nearest\").item()  + offset*zoom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc33b8-99ff-4c46-a2d2-d01eb8a16bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## in SeasFire Datacube v2.0 the burned area variables would have the water bodies masked with ERA-5 land sea mask \n",
    "mask= ds['lsm']\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2782e3-163e-40a0-8179-8b59932da82c",
   "metadata": {},
   "source": [
    "Find the biggest forest fire during the last three years within that place\n",
    "\n",
    "'2018-01-01'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d33b7a-61a3-468a-9759-9101d627d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwis_aoi = gwis.sel(time=slice(datetime(2018,1,1),datetime(2021,1,1)), latitude=slice(lat_start, lat_stop),longitude=slice(lon_start, lon_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f71db-2fad-4be2-8e6a-5dafc48ee6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateind_max_fire = gwis_aoi.sum(dim={'latitude','longitude'}).argmax(dim='time').compute().item()\n",
    "fire_date = gwis_aoi.time[dateind_max_fire]\n",
    "# where the sum in the plot is the highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c1f26e-f027-4afb-b4a9-c2bbc8fb6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idxmax check out---> same thing with argmax + sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878ecc3c-103a-4b1f-a1df-3639df6bde70",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_max_fire = gwis_aoi.isel(time=dateind_max_fire)\n",
    "date_max_fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b95b98-5dcb-449c-bb80-d9abc4648ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gwis_all=gwis.resample(time=\"1Y\").sum()\n",
    "gwis_all=date_max_fire\n",
    "gwis_all= gwis_all.where(mask)\n",
    "gwis_all.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7711693-4072-43f8-b44a-84b22302d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cartopy\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cartopy.crs as ccrs\n",
    "\n",
    "\n",
    "\n",
    "# da = gwis_all\n",
    "\n",
    "# fig, ax = plt.subplots(\n",
    "#     figsize=(8, 6),\n",
    "#     subplot_kw={\"projection\": ccrs.PlateCarree()}\n",
    "# )\n",
    "\n",
    "# da.plot(\n",
    "#     ax=ax,\n",
    "#     transform=ccrs.PlateCarree(),\n",
    "#     cmap=\"viridis\",\n",
    "#     cbar_kwargs={\"label\": \"Tâ‚‚â‚˜ (K)\"}\n",
    "# )\n",
    "\n",
    "# ax.coastlines(resolution=\"10m\", color=\"black\", linewidth=1)\n",
    "# ax.add_feature(cartopy.feature.BORDERS, linewidth=0.5)\n",
    "\n",
    "# ax.set_extent([lon_start, lon_stop, lat_stop, lat_start], crs=ccrs.PlateCarree())\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2578e-c9e7-4a8f-a06c-cf5cdd5656b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "\n",
    "# Plot it interactively\n",
    "gwis_all.hvplot(\n",
    "    x='longitude',\n",
    "    y='latitude',\n",
    "    cmap='viridis',\n",
    "    colorbar=True,\n",
    "    frame_width=600,\n",
    "    frame_height=400,\n",
    "    geo=True,\n",
    "    tiles='OSM'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb06b99-6846-4cf1-97e3-d0d5818f258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "\n",
    "# lat_start=41\n",
    "# lat_stop=41.5\n",
    "# lon_start=-7.7\n",
    "# lon_stop=-7.5\n",
    "\n",
    "# lat_slice=slice(41.5,41)\n",
    "# lon_slice=slice(-7.7,-7.5)\n",
    "\n",
    "# fires = gwis_all.sel(latitude=lat_slice,longitude=lon_slice)\n",
    "fire_date_t = pd.to_datetime(fire_date.values.item())\n",
    "\n",
    "week_before = (fire_date_t - timedelta(days=7))\n",
    "week_after = (fire_date_t + timedelta(days=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcea67e-ae5e-4764-bbdf-58541a72688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(week_before.date(), \"---\" , week_after.date())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db1ed9-11d1-4f9c-9bf4-d8fc6847e9fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac0f4f-2989-4e50-9a84-839e935a4a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pystac_client import Client as pystac_client\n",
    "# from odc.stac import configure_rio, stac_load\n",
    "\n",
    "\n",
    "# Open a catalog\n",
    "# catalog = pystac_client.open(\"https://earth-search.aws.element84.com/v1\")\n",
    "# time_start = \"2020-07-01\"\n",
    "# time_end = \"2020-07-28\"\n",
    "# time_range = time_start+\"/\"+time_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f37ae-d276-4d61-b403-1a6d58e29511",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c607a81-ba61-485b-9d7b-7dfa779b21a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'NBR'\n",
    "\n",
    "bandnames_dict = {\n",
    "    'nir': 'nir',\n",
    "    'swir22': 'swir22'\n",
    "}\n",
    "crs = \"epsg:\"+ str(epsg)  # projection on which the data will be projected\n",
    "\n",
    "# km2deg = 1.0 / 111\n",
    "# x, y = (23.9983519, 37.7351433)  # Center point of a query\n",
    "# r = 4 * km2deg  \n",
    "# bbox = (x - r, y - r, x + r, y + r)\n",
    "# zoom = 1\n",
    "\n",
    "\n",
    "# Normalised Burn Ratio, Lopez Garcia 1991\n",
    "def calc_nbr(ds):\n",
    "    return (ds.nir - ds.swir22) / (ds.nir + ds.swir22)\n",
    "\n",
    "index_dict = {'NBR': calc_nbr}\n",
    "index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e746ce-8b40-4299-b06e-173ff0743e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pystac_client.open(\"https://earth-search.aws.element84.com/v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e7083-ebb2-4022-967a-2541e1eeb53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom=1/2\n",
    "chunk={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08b52ea-30cb-41c1-9236-cf35f6e94f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_before_start = (week_before - timedelta(days=30))\n",
    "time_range = str(week_before_start.date()) + \"/\" + str(week_before.date())\n",
    "\n",
    "query1 = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"], datetime=time_range, limit=100,\n",
    "    bbox=bbox, query={\"eo:cloud_cover\": {\"lt\": 0.5}}\n",
    ")\n",
    "\n",
    "items = list(query1.get_items())\n",
    "print(f\"Found: {len(items):d} datasets\")\n",
    "\n",
    "items_pre = min(items, key=lambda item: item.properties[\"eo:cloud_cover\"])\n",
    "\n",
    "prefire_ds = stac_load(\n",
    "    [items_pre],\n",
    "    bands=(\"nir\", \"swir22\"),\n",
    "    # crs=crs,\n",
    "    # resolution=  10*zoom,\n",
    "    chunks=chunk,  # <-- use Dask\n",
    "    groupby=\"datetime\",\n",
    "    bbox=bbox,\n",
    ")\n",
    "prefire_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb6435-2d94-4cf2-b821-e438ba494702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7055b-5782-4af6-9c10-94819659ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_after_end = (week_after + timedelta(days=30))\n",
    "time_range = str(week_after.date()) + \"/\" + str(week_after_end.date())\n",
    "\n",
    "query2 = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"], datetime=time_range, limit=100,\n",
    "    bbox=bbox, query={\"eo:cloud_cover\": {\"lt\": 0.5}}\n",
    ")\n",
    "\n",
    "items = list(query2.get_items())\n",
    "print(f\"Found: {len(items):d} datasets\")\n",
    "\n",
    "items_post = min(items, key=lambda item: item.properties[\"eo:cloud_cover\"])\n",
    "\n",
    "postfire_ds = stac_load(\n",
    "    [items_post],\n",
    "    bands=(\"nir\", \"swir22\"),\n",
    "    # crs=crs,\n",
    "    # resolution=10 * zoom,\n",
    "    chunks=chunk,  # <-- use Dask\n",
    "    groupby=\"datetime\",\n",
    "    bbox=bbox,\n",
    ")\n",
    "postfire_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e71a9a-e330-4739-8fe0-5b4df1795b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4362690-e4ab-4074-9095-672edb85787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename bands in dataset to use simple names \n",
    "bands_to_rename = {\n",
    "    a: b for a, b in bandnames_dict.items() if a in prefire_ds.variables\n",
    "}\n",
    "\n",
    "# prefire\n",
    "prefire_ds[index_name] = index_dict[index_name](prefire_ds.rename(bands_to_rename) / 10000.0)\n",
    "\n",
    "# postfire\n",
    "postfire_ds[index_name] = index_dict[index_name](postfire_ds.rename(bands_to_rename) / 10000.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f594ea4-3091-48fd-9a70-8f26fdd94054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate delta NBR\n",
    "prefire_burnratio = prefire_ds.NBR.isel(time=0)\n",
    "postfire_burnratio = postfire_ds.NBR.isel(time=0)\n",
    "\n",
    "delta_NBR = prefire_burnratio - postfire_burnratio\n",
    "\n",
    "dnbr_dataset = delta_NBR.to_dataset(name='delta_NBR').persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e1aca-5885-4f86-8d50-951ed9b2182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnbr_dataset\n",
    "delta_NBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358844d-1afc-4d5d-aa68-eb07c39b029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=[7, 10])\n",
    "\n",
    "# We're using cartopy and are plotting in PlateCarree projection \n",
    "# (see documentation on cartopy)\n",
    "ax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "#ax.set_extent([-180, 180, -70, 70], crs=ccrs.PlateCarree()) # lon1 lon2 lat1 lat2\n",
    "ax.coastlines(resolution='10m')\n",
    "ax.gridlines(draw_labels=True)\n",
    "\n",
    "# We need to project our data to the new Orthographic projection and for this we use `transform`.\n",
    "# we set the original data projection in transform (here Mercator)\n",
    "prefire_burnratio.plot(ax=ax, transform=ccrs.epsg(prefire_burnratio.spatial_ref.values), cmap='RdBu_r',\n",
    "                       cbar_kwargs={'orientation':'horizontal','shrink':0.95})\n",
    "\n",
    "# One way to customize your title\n",
    "plt.title( pd.to_datetime(prefire_burnratio.time.values.item()).strftime(\"%d %B %Y\"), fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ddf95-672f-432f-b1b2-047b4f4a54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=[7, 9])\n",
    "\n",
    "# We're using cartopy and are plotting in PlateCarree projection \n",
    "# (see documentation on cartopy)\n",
    "ax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "#ax.set_extent([-180, 180, -70, 70], crs=ccrs.PlateCarree()) # lon1 lon2 lat1 lat2\n",
    "ax.coastlines(resolution='10m')\n",
    "ax.gridlines(draw_labels=True)\n",
    "\n",
    "# We need to project our data to the new Orthographic projection and for this we use `transform`.\n",
    "# we set the original data projection in transform (here Mercator)\n",
    "postfire_burnratio.plot(ax=ax, transform=ccrs.epsg(postfire_burnratio.spatial_ref.values), cmap='RdBu_r',\n",
    "                        cbar_kwargs={'orientation':'horizontal','shrink':0.95})\n",
    "\n",
    "# One way to customize your title\n",
    "plt.title( pd.to_datetime(postfire_burnratio.time.values.item()).strftime(\"%d %B %Y\"), fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14caa7b7-7026-4039-8da4-02a1394d40ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=[7, 10])\n",
    "\n",
    "# We're using cartopy and are plotting in PlateCarree projection \n",
    "# (see documentation on cartopy)\n",
    "ax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "ax.coastlines(resolution='10m')\n",
    "ax.gridlines(draw_labels=True)\n",
    "\n",
    "# We need to project our data to the new Orthographic projection and for this we use `transform`.\n",
    "# we set the original data projection in transform (here Mercator)\n",
    "dnbr_dataset.delta_NBR.plot(ax=ax, transform=ccrs.epsg(dnbr_dataset.delta_NBR.spatial_ref.values), cmap='RdBu_r',\n",
    "                            cbar_kwargs={'orientation':'horizontal','shrink':0.95})\n",
    "\n",
    "# One way to customize your title\n",
    "plt.title( \"Delta NBR\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ce2a3-d72c-4d98-886a-6765a7cde16b",
   "metadata": {},
   "source": [
    "https://un-spider.org/advisory-support/recommended-practices/recommended-practice-burn-severity/in-detail/normalized-burn-ratio\n",
    "\n",
    "![img](https://un-spider.org/sites/default/files/table+legend.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceebd22-b673-4bee-9489-d7f22904fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "BURN_THRESH = 0.27\n",
    "burn_mask = dnbr_dataset.delta_NBR > BURN_THRESH           # True/False mask, same shape as raster\n",
    "burn_mask.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e1af97-4521-471c-99fb-6e934e2a3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx, dy = dnbr_dataset.delta_NBR.rio.resolution()\n",
    "pixel_area_ha = abs(dx * dy) / 1e4       # 10m Ã— 10m  â†’ 0.01 ha\n",
    "pixel_area_ha\n",
    "\n",
    "pixels_burned   = burn_mask.sum().compute().item()   # integer number of burned pixels\n",
    "burned_area_ha  = pixels_burned * pixel_area_ha\n",
    "\n",
    "print(f\"Pixels burned : {pixels_burned:,d}\")\n",
    "print(f\"Burned area   : {burned_area_ha:,.2f} ha\")\n",
    "print(f\"Actual Burned Area : {gwis_all.sum().compute():,.2f}, ha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16bcf9a-71cf-46b0-8a3b-bf72b3e7d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnbr_dataset['burned_ha_mask'] = burn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d402ab-41cd-4dba-b5d2-39f5d949c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnbr_dataset = dnbr_dataset.persist()\n",
    "gwis_all = gwis_all.persist()\n",
    "\n",
    "\n",
    "# r_dnbr_dataset = dnbr_dataset.rename({'x': 'lon', 'y': 'lat'})\n",
    "# r_gwis_all = gwis_all.rename({'longitude': 'lon', 'latitude': 'lat'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e24d9f-d8a8-4879-9145-68327b0268bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwis_all = gwis_all.rio.write_crs(ds.rio.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda89645-3469-4c29-81de-f4d5a020cb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dea004b6-30dc-4ffa-a20b-32f1abd6deed",
   "metadata": {},
   "source": [
    "Plot is off because of bad projection (curcilinear to rectilinear) but we can see that generally the fires are in the north-west/north-east regions with two distinct occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f1959-938a-41d9-8673-7f030608e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "\n",
    "gwis_all_reprojected = gwis_all.rio.reproject(dnbr_dataset.delta_NBR.rio.crs)\n",
    "\n",
    "dnbr_plot = dnbr_dataset.delta_NBR.hvplot(\n",
    "    width=700,\n",
    "    height=700,\n",
    "    title='dNBR (10 m) with GWIS overlay',\n",
    "    alpha=1.0\n",
    ")\n",
    "\n",
    "# Plot the reprojected coarse dataset as transparent overlay\n",
    "gwis_plot = gwis_all_reprojected.hvplot(\n",
    "    cmap='Reds',\n",
    "    alpha=0.3,\n",
    "    clim=(0, gwis_all.max().compute().item())\n",
    ")\n",
    "\n",
    "# Combine them interactively\n",
    "combined_plot = dnbr_plot * gwis_plot\n",
    "\n",
    "combined_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2508b-51de-40b7-9268-55352aebb8b3",
   "metadata": {},
   "source": [
    "# Saving Your Work\n",
    "\n",
    "xrlint and validate your cube..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87fe44-6081-4600-8a49-59d6596596a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xrlint.all as xrl\n",
    "\n",
    "linter = xrl.new_linter(\"recommended\")\n",
    "linter.validate(dnbr_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27975471-0a3b-4a8d-ae9d-0cbe281fe265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign dataset-level attributes\n",
    "dnbr_dataset.attrs.update({\n",
    "    'title': 'Delta NBR and Burned Area Mask Dataset',\n",
    "    'history': 'Created by reprojecting and aligning datasets for fire severity analysis',\n",
    "    'Conventions': 'CF-1.7'\n",
    "})\n",
    "\n",
    "\n",
    "# Assign variable-level attributes for delta_NBR\n",
    "dnbr_dataset.delta_NBR.attrs.update({\n",
    "    'institution': 'Lampata',\n",
    "    'source': 'Sentinel-2 imagery; processed with open-source dNBR code, element84...',\n",
    "    'references': 'https://example.com/ref',\n",
    "    'comment': 'dNBR values represent change in vegetation severity post-fire',\n",
    "    'standard_name': 'difference_normalized_burn_ratio',\n",
    "    'long_name': 'Differenced Normalized Burn Ratio (dNBR)',\n",
    "    'units': 'm'\n",
    "})\n",
    "\n",
    "# Example for burned_ha_mask data variable\n",
    "dnbr_dataset.burned_ha_mask.attrs.update({\n",
    "    'standard_name': 'burned_area_mask',\n",
    "    'long_name': 'Burned Area Mask in Hectares',\n",
    "    'units': 'hectares',\n",
    "    'institution': 'Your Institution Name',\n",
    "    'source': 'Derived from wildfire impact analysis',\n",
    "    'references': 'https://example.com/ref',\n",
    "    'comment': 'Burned area mask showing presence of burned areas'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915b6804-6c2a-4e4f-b31e-110b34bea418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcube.core.verify import assert_cube\n",
    "\n",
    "assert_cube(dnbr_dataset)  # raises ValueError if it's not xcube-valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaafa64b-614f-4fa4-9e61-5e47a2804ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add time\n",
    "dnbr_dataset = dnbr_dataset.expand_dims(time=[postfire_ds.time.isel(time=0).values])\n",
    "dnbr_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6396c-f8f7-4500-9b75-1f6ea8b1b134",
   "metadata": {},
   "source": [
    "Chunk Data for Better Usability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2dd0c-4cca-4aad-8dc9-d1f6161670dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnbr_dataset = dnbr_dataset.chunk({\"time\": 1, \"y\": 1000, \"x\": 1000})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b937fa-a556-4d96-a09c-8a946a1a2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(dnbr_dataset.burned_ha_mask.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e699c-3b6b-480f-903a-7d957dfdd062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_at_folder = './wildfires'\n",
    "if not os.path.exists(save_at_folder):\n",
    "    os.makedirs(save_at_folder)\n",
    "\n",
    "# Define the output path within your notebook folder\n",
    "output_path = os.path.join(save_at_folder, \"dnbr_dataset.zarr\")\n",
    "\n",
    "# save\n",
    "dnbr_dataset.to_zarr(output_path, mode=\"w\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo] *",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
