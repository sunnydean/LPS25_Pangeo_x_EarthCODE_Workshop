{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1da4c8-5f6e-4630-880d-2c789435afe1",
   "metadata": {},
   "source": [
    "---\n",
    "title: Dask 101\n",
    "subtitle: D.03.10 HANDS-ON TRAINING - EarthCODE 101 Hands-On Workshop - Example showing how to access data on the EarthCODE Open Science Catalog and working with the Pangeo ecosystem on EDC\n",
    "authors:\n",
    "  - name: Deyan Samardzhiev\n",
    "    github: sunnydean\n",
    "    orcid: 0009-0003-3803-8522\n",
    "    affiliations:\n",
    "      - id: Lampata UK\n",
    "        institution: Lampata UK\n",
    "reviewers:\n",
    "  - name: Anne Fouilloux\n",
    "    orcid: 0000-0002-1784-2920\n",
    "    github: annefou\n",
    "    affiliations:\n",
    "      - id: Simula Research Laboratory\n",
    "        institution: Simula Research Laboratory\n",
    "        ror: 00vn06n10\n",
    "date: 2025-06-01\n",
    "thumbnail: https://raw.githubusercontent.com/ESA-EarthCODE/documentation/refs/heads/main/pages/public/img/EarthCODE_kv_transparent.png\n",
    "keywords: [\"earthcode\", \"pangeo\", \"stac\", \"xarray\", \"earth observation\", \"remote sensing\"]\n",
    "tags: [\"pangeo\"]\n",
    "releaseDate: 2025-06-01\n",
    "datePublished: 2025-06-01\n",
    "dateModified: 2025-06-01\n",
    "banner: ../static/PANGEO.png\n",
    "github: https://github.com/sunnydean/LPS25_Pangeo_x_EarthCODE_Workshop\n",
    "license: MIT\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2816996c-cff3-4b31-b5ab-c9431a347410",
   "metadata": {},
   "source": [
    "# Context\n",
    "We will be using Dask with Xarray to parallelize our data analysis. The analysis is very similar to what we have done in previous examples but this time we will use data on a global coverage that we read from the SeasFire Cube.\n",
    "\n",
    "We will learn how to use the EDC Pangeo Dask Gateway to analyse data at scale.\n",
    "\n",
    "### Data\n",
    "\n",
    "In this workshop, we will be using the [SeasFire Data Cube](https://opensciencedata.esa.int/products/seasfire-cube/collection) published to the EarthCODE Open Science Catalog\n",
    "\n",
    "#### Related publications\n",
    "* *Alonso, Lazaro, Gans, Fabian, Karasante, Ilektra, Ahuja, Akanksha, Prapas, Ioannis, Kondylatos, Spyros, Papoutsis, Ioannis, Panagiotou, Eleannna, Michail, Dimitrios, Cremer, Felix, Weber, Ulrich, & Carvalhais, Nuno. (2022). SeasFire Cube: A Global Dataset for Seasonal Fire Modeling in the Earth System (0.4) [Data set]. Zenodo. @alonso-2024. The same dataset can also be downloaded from Zenodo: https://zenodo.org/records/13834057*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e4e45-72c9-446e-b65a-f72c86cda407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.distributed\n",
    "import xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f41a83-633d-4c47-9da2-ac5ba8ae8d9b",
   "metadata": {},
   "source": [
    "## Parallelize with Dask\n",
    "\n",
    "We know from previous chapter [cloud native formats 101](./cloud-native-formats101.ipynb) that chunking is key for analyzing large datasets. In this episode, we will learn to parallelize our data analysis using [Dask](https://docs.dask.org/) on our chunked dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdaf18d-7e0e-4965-9063-e290ccce926d",
   "metadata": {},
   "source": [
    "### What is [Dask](https://docs.dask.org/) ?\n",
    "\n",
    "**Dask** scales the existing Python ecosystem: with very or no changes in your code, you can speed-up computation using Dask or process bigger than memory datasets.\n",
    "\n",
    "- Dask is a flexible library for parallel computing in Python.\n",
    "- It is widely used for handling large and complex Earth Science datasets and speed up science.\n",
    "- Dask is powerful, scalable and flexible. It is the leading platform today for data analytics at scale.\n",
    "- It scales natively to clusters, cloud, HPC and bridges prototyping up to production.\n",
    "- The strength of Dask is that is scales and accelerates the existing Python ecosystem e.g. Numpy, Pandas and Scikit-learn with few effort from end-users.\n",
    "\n",
    "It is interesting to note that at first, [Dask has been created to handle data that is larger than memory, on a single computer](https://coiled.io/blog/history-dask/). It then was extended with Distributed to compute data in parallel over clusters of computers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57dc665-107a-492b-ab8a-36d1d6692be2",
   "metadata": {},
   "source": [
    "#### How does Dask scale and accelerate your data analysis?\n",
    "\n",
    "[Dask proposes different abstractions to distribute your computation](https://docs.dask.org/en/stable/10-minutes-to-dask.html). In this _Dask Introduction_ section, we will focus on [Dask Array](https://docs.dask.org/en/stable/array.html) which is widely used in pangeo ecosystem as a back end of Xarray.\n",
    "\n",
    "As shown in the [previous section](./chunking_introduction.ipynb) Dask Array is based on chunks.\n",
    "Chunks of a Dask Array are well-known Numpy arrays. By transforming our big datasets to Dask Array, making use of chunk, a large array is handled as many smaller Numpy ones and we can compute each of these chunks independently.\n",
    "\n",
    "![Dask and Numpy](https://examples.dask.org/_images/dask-array-black-text.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c938e90-cac9-4739-b34c-2fd4bea6119b",
   "metadata": {},
   "source": [
    "#### How does Xarray with Dask distribute data analysis?\n",
    "\n",
    "When we use chunks with `Xarray`, the real computation is only done when needed or asked for, usually when invoking `compute()` or `load()` functions. Dask generates a **task graph** describing the computations to be done. When using [Dask Distributed](https://distributed.dask.org/en/stable/) a **Scheduler** distributes these tasks across several **Workers**.\n",
    "\n",
    "![Xarray with da sk](../static/dask-xarray-explained.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3667054c-c61d-4724-8514-f060ac493dc2",
   "metadata": {},
   "source": [
    "### What is a Dask Distributed cluster ?\n",
    "\n",
    "A Dask Distributed cluster is made of two main components:\n",
    "\n",
    "- a Scheduler, responsible for handling computations graph and distributing tasks to Workers.\n",
    "- One or several (up to 1000s) Workers, computing individual tasks and storing results and data into distributed memory (RAM and/or worker's local disk).\n",
    "\n",
    "A user usually needs __Client__ and __Cluster__ objects as shown below to use Dask Distributed.    \n",
    "\n",
    "![Dask Distributed Cluster](https://user-images.githubusercontent.com/306380/66413985-27111600-e9be-11e9-9995-8f418ff48f8a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a5216e-7b27-41ab-b55f-dbcd4928d047",
   "metadata": {},
   "source": [
    "#### Where can we deploy a Dask distributed cluster?\n",
    "\n",
    "\n",
    "[Dask distributed clusters can be deployed on your laptop or on distributed infrastructures (Cloud, HPC centers, Hadoop, etc.)](https://docs.dask.org/en/stable/deploying.html)  Dask distributed `Cluster` object is responsible of deploying and scaling a Dask Cluster on the underlying resources.\n",
    "\n",
    "EDC has one such deployment\n",
    "\n",
    "\n",
    "![Dask Cluster deployment](https://docs.dask.org/en/stable/_images/dask-cluster-manager.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5b958-486c-42c3-91e7-31677267c411",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "A Dask `Cluster` can be created on a single machine (for instance your laptop) e.g. there is no need to have dedicated computational resources. However, speedup will only be limited to your single machine resources if you do not have dedicated computational resources!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e779ac9-92a5-4afd-886f-4fa5a5fa35a6",
   "metadata": {},
   "source": [
    "### Dask distributed Client\n",
    " \n",
    "The Dask distributed `Client` is what allows you to interact with Dask distributed Clusters. When using Dask distributed, you always need to create a `Client` object. Once a `Client` has been created, it will be used by default by each call to a Dask API, even if you do not explicitly use it.\n",
    "\n",
    "No matter the Dask API (e.g. Arrays, Dataframes, Delayed, Futures, etc.) that you use, under the hood, Dask will create a Directed Acyclic Graph (DAG) of tasks by analysing the code. Client will be responsible to submit this DAG to the Scheduler along with the final result you want to compute. The Client will also gather results from the Workers, and aggregate it back in its underlying Python process.\n",
    "\n",
    "Using `Client()` function with no argument, you will create a local Dask cluster with a number of workers and threads per worker corresponding to the number of cores in the 'local' machine. Here, during the workshop, we are running this notebook in the EDC Pangeo cloud deployment, so the 'local' machine is the jupyterlab you are using at the Cloud, and the number of cores is the number of cores on the cloud computing resources you've been given (not on your laptop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf8cd7-66c3-4438-9554-deb2b8708371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()   # create a local dask cluster on the local machine.\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8143cc9-eff0-4328-90d0-2911c9a3e81f",
   "metadata": {},
   "source": [
    "Inspecting the `Cluster Info` section above gives us information about the created cluster: we have 2 or 4 workers and the same number of threads (e.g. 1 thread per worker). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3246a58-39b3-4e6b-9d65-71740f3ffe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close client to clean resources\n",
    "# Note, you can run this tutorial locally if you uncomment this line \n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5882f97a-4604-4eaf-8b7c-7e8e116328bd",
   "metadata": {},
   "source": [
    "## Scaling your Computation using Dask Gateway.\n",
    "\n",
    "For this workshop, you will learn how to use Dask Gateway to manage Dask clusters over Kubernetes, allowing to run our data analysis in parallel e.g. distribute tasks across several workers.\n",
    "\n",
    "Dask Gateway is a component that helps you manage and create Dask Clusters across your environment.\n",
    "As Dask Gateway is configured by default on this infrastructure, you just need to execute the following cells.\n",
    "\n",
    "Note that, if you're executing this locally, without prior setup for dask gateway, the following code will crash. If you would like to run this locally, skip these lines and continue with the local dask client above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389ad4f2-515b-4831-94ad-f126bb815bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "gateway = Gateway()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04427ff5-215d-4378-a330-3f20ecbbf624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING In case you already created gateway cluster, you will see list of your clusters. \n",
    "#And this cell will kill all your orphan clusters.\n",
    "#Please clean them before you make a new cluster using following command \n",
    "clusters = gateway.list_clusters()\n",
    "print(clusters)\n",
    "\n",
    "for cluster in clusters:\n",
    "    cluster = gateway.connect(cluster.name)\n",
    "    cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfda06d-541a-401b-9618-6c4fd3ad3620",
   "metadata": {},
   "source": [
    "## EDC Dask Gateway Options\n",
    "\n",
    "EDC Pangeo's Dask Gateway provides several cluster configurations for your workloads. You can choose the appropriate size based on your computational needs:\n",
    "\n",
    "- **`small`**  \n",
    "  - **Worker cores:** 0.5  \n",
    "  - **Worker memory:** 1 GB  \n",
    "\n",
    "- **`medium`**  \n",
    "  - **Worker cores:** 2  \n",
    "  - **Worker memory:** 2 GB  \n",
    "\n",
    "- **`larger`**  \n",
    "  - **Worker cores:** 4  \n",
    "  - **Worker memory:** 4 GB  \n",
    "\n",
    "You can set these options when spawning your Dask clusters to ensure optimal resource allocation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d9500-0dfb-40dc-9690-68b5b534d513",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_options = gateway.cluster_options()\n",
    "cluster_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a454cef-1dc6-4c55-98a5-b33cbd1ebf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = gateway.new_cluster(cluster_options=cluster_options)\n",
    "cluster.scale(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bf2b6-a467-4c37-b93b-becf66dc4d97",
   "metadata": {},
   "source": [
    "### Get a client from the Dask Gateway Cluster\n",
    "\n",
    "As stated above, creating a Dask `Client` is mandatory in order to perform following Daks computations on your Dask Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d454cf-da47-4ecf-a1e9-f8e919755e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3e8efd-c547-4bcd-b625-a6e4e29e177c",
   "metadata": {},
   "source": [
    "### Dask Dashboard\n",
    "\n",
    "Dask comes with a really handy interface: the Dask Dashboard. It is a web interface that you can open in a separated tab of your browser.\n",
    "\n",
    "We will learn here how to use it through [dask jupyterlab extension](https://github.com/dask/dask-labextension).  \n",
    "\n",
    "To use Dask Dashboard through jupyterlab extension on Pangeo EDC infrastructure,\n",
    "you will just need too look at the html link you have for your jupyterlab, and Dask dashboard port number, as highlighted in the figure below.\n",
    "\n",
    "![Dash Board link](../static/dashboardlink.png)\n",
    "![Dash lab](../static/dasklab.png)\n",
    "\n",
    "Then click the orange icon indicated in the above figure, and type 'your' dashboard link (normally, you just need to replace 'todaka' to 'your username').  \n",
    "\n",
    "\n",
    "You can click several buttons indicated with blue arrows in above figures, then drag and drop to place them as your convenience.  \n",
    "\n",
    "![Example dask lab](../static/exampledasklab.png)\n",
    "\n",
    "\n",
    "It's really helpfull to understand your computation and how it is distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57cb6a0-df4c-4625-9a09-83a73034d782",
   "metadata": {},
   "source": [
    "## Dask Distributed computations on our dataset\n",
    "\n",
    "Let's open the SeasFire dataset we previously looked at, select a single location over time, visualize the task graph generated by Dask, and observe the Dask Dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946cbae7-14fe-4672-a871-434df473d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "http_url = \"https://s3.waw4-1.cloudferro.com/EarthCODE/OSCAssets/seasfire/seasfire_v0.4.zarr/\"\n",
    "\n",
    "ds = xarray.open_dataset(\n",
    "\thttp_url,\n",
    "\tengine='zarr',\n",
    "    chunks={},\n",
    "\tconsolidated=True\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95a75e3-3384-4a76-9b52-7f29e32d876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask= ds['lsm'][:,:]\n",
    "gwis_all= ds.gwis_ba.resample(time=\"1YE\").sum()\n",
    "gwis_all= gwis_all.where(mask>0.5)\n",
    "\n",
    "gwis_2020= gwis_all.sel(time='2020-08-01', method='nearest')\n",
    "gwis_2020.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c1b58e-3e60-4b91-a48d-bf76bf6ca3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwis_2020.data.visualize(optimize_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7627f748-8799-4add-b670-475ba5627393",
   "metadata": {},
   "source": [
    "Did you notice something on the Dask Dashboard when running the two previous cells?\n",
    "\n",
    "We didn't 'compute' anything. We just built a Dask task graph with it's size indicated as count above, but did not ask Dask to return a result.\n",
    "\n",
    "Note that underneath, dask optimizes the execution graph (opaquely), so as to minimize overheads and overall execution resources (hence why we're passing optimize_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ab9df-bad6-4837-9ce1-158b9e358e64",
   "metadata": {},
   "source": [
    "# Computing\n",
    "\n",
    "Calling compute on our Xarray object will trigger the execution on the Dask Cluster. Alternatively any action that would demand the computation of our data (e.g. plotting) would trigger the execution of our workflow.\n",
    "\n",
    "You should be able to see how Dask is working on Dask Dashboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5465494c-7ea3-4900-92bb-c6577ec1d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwis_2020.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo] *",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
